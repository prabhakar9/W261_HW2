{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# DATASCI W261: Machine Learning at Scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Name: Prabhakar Gundugola <br>\n",
    "Email: prabhakar@berkeley.edu <br>\n",
    "Time of Initial Submission: Jan 26, 2016 <br>\n",
    "W261-3: Spring 2016 <br>\n",
    "Week 2: Homework 2 <br>\n",
    "Date: January 26, 2016 <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW2.0\n",
    "#### What is a race condition in the context of parallel computation? Give an example.\n",
    "A race condition is a special condition that can occur when two or more threads can access shared data and they try to change it at the same time.\n",
    "\n",
    "![Race condition screenshot](racecondition.png)\n",
    "\n",
    "As shown in the above diagram, consider process A and process B access the same variable filename in slot 7. Initially process B updates the filename variable with \"ProgB.c\" in slot 7. If process A writes its filename to slot 7 which erases process B's filename, then process B will thrown an error.\n",
    "\n",
    "#### What is MapReduce?\n",
    "MapReduce is an **embarrassingly parallel** framework for processing and generating large datasets with a parallel, distributed algorithm on a cluster. \n",
    "\n",
    "A MapReduce program is composed of 2 procedures:\n",
    "- Map: Procedure that processes to generate a set of intermediate key/value pairs\n",
    "- Reduce: Procedure that performs a summary operation on all intermediate values associated with the same intermediate key.\n",
    "\n",
    "MapReduce programs written in the above functional style are automatically parallelized and executed on a large number of commodity machines. \n",
    "\n",
    "#### How does it differ from Hadoop?\n",
    "Hadoop is a software platform that implements the MapReduce programming paradigm. It also provides the run-time system that takes care of the details of partitioning of input data, scheduling the program's execution across a set of machines, handling machine failures, and managing the required inter-machine communication.\n",
    "\n",
    "#### Which programming paradigm is Hadoop based on? \n",
    "Hadoop is based on MapReduce programming model, which is embarrassingly parallel. A problem is considered to be embarrassingly parallel if there is little or no effort required to split the problem into a number of parallel tasks.\n",
    "\n",
    "#### Explain and give a simple example in code and show the code running?\n",
    "- Hadoop MapReduce divides single large data processing job into a number of parallel small map tasks. Once the tasks have been created, they are spread across multiple nodes and are run in parallel. Each of these map tasks generate a set of key/value pairs. \n",
    "- It then implements the barrier and sorts all the key/value pairs by key.\n",
    "- The framework then distributes the keys to reduce jobs based on a hash function.\n",
    "- The reduce jobs processes the input data from barrier and generate the output, which is the output of the MapReduce program.\n",
    "\n",
    "A typical example of MapReduce algorithm is find the word count in a given list of files and then print the top 10 frequent words. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MapReduce Example - Map\n",
    "- This map procedure reads files from local disk line by line.\n",
    "- It then breaks each line into words and prints the key, value pair (word, 1) for each word in a different file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper_example.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper_example.py\n",
    "#!/usr/bin/python\n",
    "import os\n",
    "import re\n",
    "WORD_RE = re.compile(r\"[\\w]+\")\n",
    "\n",
    "files = [f for f in os.listdir(\"/root/hw2\") if \"wordcount\" in f]\n",
    "\n",
    "for f in files:\n",
    "    with open(f, \"r\") as fp:\n",
    "        for line in fp.readlines():\n",
    "            for word in WORD_RE.findall(line):\n",
    "                print '{0}\\t{1}'.format(word.lower(), str(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MapReduce Example - Map\n",
    "- This reduce procedure takes input from mapper in the form of key, value pair .\n",
    "- It then accumulates the count for each key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer_example.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer_example.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "\n",
    "def wcount(prev_word, count):\n",
    "    if prev_word is not None:\n",
    "        print prev_word.ljust(20) + \"\\t\" + str(count)\n",
    "\n",
    "prev_word = None\n",
    "count = 0\n",
    "for line in sys.stdin:\n",
    "    word, value = line.split('\\t', 1)\n",
    "    if word != prev_word:\n",
    "        wcount(prev_word, count)\n",
    "        prev_word = word\n",
    "        count = 0\n",
    "    count += eval(value)\n",
    "wcount(prev_word, count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MapReduce Example - Printing the top 10 frequent words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a                   \t8\r\n",
      "state               \t5\r\n",
      "output              \t4\r\n",
      "for                 \t4\r\n",
      "of                  \t4\r\n",
      "system              \t3\r\n",
      "inputs              \t3\r\n",
      "may                 \t3\r\n",
      "to                  \t3\r\n",
      "hi                  \t3\r\n"
     ]
    }
   ],
   "source": [
    "!python mapper_example.py | sort -k1,1 | python reducer_example.py| sort -rk2,2 | head -10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW2.1. Sort in Hadoop MapReduce\n",
    "Given as input: Records of the form <integer, “NA”>, where integer is any integer, and “NA” is just the empty string. <br>\n",
    "Output: sorted key value pairs of the form <integer, “NA”> in decreasing order. <br>\n",
    "#### What happens if you have multiple reducers? Do you need additional steps? Explain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Hadoop MapReduce, MapReduce programming model relies on the sorting feature of Hadoop framework between mappers and reducers.\n",
    "\n",
    "If there are multiple reducers, each reducer will generate outputs/keys that are sorted within each reducer, but not sorted across all reducers. \n",
    "\n",
    "Additional steps are required to sort the consolidated key value pairs. They are:\n",
    "- Sort the multiple reducers output of key, value pairs and then pass this as input to a single reducer.\n",
    "\n",
    "Another way to achieve this is having a combiner in between the Map and Reduce procedures. A combiner, also known as a semi-reducer, accepts the inputs from the Map procedure and thereafter passes the output of key,value pairs to the Reduce procedure\n",
    "\n",
    "![Combiner - Multiple reducers](combiner.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####Write code to generate N  random records of the form <integer, “NA”>. Let N = 10,000.\n",
    "Write the python Hadoop streaming map-reduce job to perform this sort. Display the top 10 biggest numbers. Display the 10 smallest numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting randomgenerator.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile randomgenerator.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "import random\n",
    "\n",
    "for i in range(10000):\n",
    "    sys.stdout.write(\"{0},{1}\\n\".format(random.randint(1, 10000), \"NA\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/25 23:58:01 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/01/25 23:58:03 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# Change permissions on randomgenerator.py\n",
    "!chmod a+x randomgenerator.py\n",
    "\n",
    "# Call randomgenerator.py to generate and output the random numbers to a file\n",
    "!./randomgenerator.py > randomrecords.txt\n",
    "\n",
    "# Copy it to HDFS\n",
    "!hdfs dfs -mkdir -p /user/root/wk2/hw21/input\n",
    "!hdfs dfs -put randomrecords.txt /user/root/wk2/hw21/input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Mapper for Hadoop streaming MapReduce job to sort the numbers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapreducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapreducer.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "for line in sys.stdin:\n",
    "    key, value = line.strip().split(',')\n",
    "    print '{0},{1}'.format(key, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Change permissions on mapreducer.py\n",
    "!chmod a+x mapreducer.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Running the MapReduce in Hadoop**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/26 00:05:24 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/01/26 00:05:25 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /user/root/wk2/hw21/output\n",
      "16/01/26 00:05:27 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/01/26 00:05:28 INFO Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id\n",
      "16/01/26 00:05:28 INFO jvm.JvmMetrics: Initializing JVM Metrics with processName=JobTracker, sessionId=\n",
      "16/01/26 00:05:28 INFO jvm.JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized\n",
      "16/01/26 00:05:28 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/01/26 00:05:28 INFO mapreduce.JobSubmitter: number of splits:1\n",
      "16/01/26 00:05:28 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local1386910411_0001\n",
      "16/01/26 00:05:29 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
      "16/01/26 00:05:29 INFO mapreduce.Job: Running job: job_local1386910411_0001\n",
      "16/01/26 00:05:29 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
      "16/01/26 00:05:29 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
      "16/01/26 00:05:29 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/01/26 00:05:29 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
      "16/01/26 00:05:29 INFO mapred.LocalJobRunner: Starting task: attempt_local1386910411_0001_m_000000_0\n",
      "16/01/26 00:05:29 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/01/26 00:05:29 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
      "16/01/26 00:05:29 INFO mapred.MapTask: Processing split: hdfs://localhost:54310/user/root/wk2/hw21/input/randomrecords.txt:0+78881\n",
      "16/01/26 00:05:29 INFO mapred.MapTask: numReduceTasks: 1\n",
      "16/01/26 00:05:29 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "16/01/26 00:05:29 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "16/01/26 00:05:29 INFO mapred.MapTask: soft limit at 83886080\n",
      "16/01/26 00:05:29 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "16/01/26 00:05:29 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "16/01/26 00:05:29 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "16/01/26 00:05:29 INFO streaming.PipeMapRed: PipeMapRed exec [/root/hw2/mapreducer.py]\n",
      "16/01/26 00:05:29 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
      "16/01/26 00:05:29 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
      "16/01/26 00:05:29 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
      "16/01/26 00:05:29 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
      "16/01/26 00:05:29 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
      "16/01/26 00:05:29 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
      "16/01/26 00:05:29 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
      "16/01/26 00:05:29 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
      "16/01/26 00:05:29 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
      "16/01/26 00:05:29 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
      "16/01/26 00:05:29 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
      "16/01/26 00:05:29 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
      "16/01/26 00:05:29 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/26 00:05:29 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/26 00:05:29 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/26 00:05:29 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/26 00:05:29 INFO streaming.PipeMapRed: R/W/S=10000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/26 00:05:29 INFO streaming.PipeMapRed: Records R/W=10000/1\n",
      "16/01/26 00:05:29 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/01/26 00:05:29 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/01/26 00:05:29 INFO mapred.LocalJobRunner: \n",
      "16/01/26 00:05:29 INFO mapred.MapTask: Starting flush of map output\n",
      "16/01/26 00:05:29 INFO mapred.MapTask: Spilling map output\n",
      "16/01/26 00:05:29 INFO mapred.MapTask: bufstart = 0; bufend = 88881; bufvoid = 104857600\n",
      "16/01/26 00:05:29 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26174400(104697600); length = 39997/6553600\n",
      "16/01/26 00:05:29 INFO streaming.PipeMapRed: PipeMapRed exec [/root/hw2/mapreducer.py]\n",
      "16/01/26 00:05:29 INFO Configuration.deprecation: mapred.skip.map.auto.incr.proc.count is deprecated. Instead, use mapreduce.map.skip.proc-count.auto-incr\n",
      "16/01/26 00:05:29 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/26 00:05:29 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/26 00:05:29 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/26 00:05:29 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/26 00:05:30 INFO streaming.PipeMapRed: R/W/S=10000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/26 00:05:30 INFO streaming.PipeMapRed: Records R/W=10000/1\n",
      "16/01/26 00:05:30 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/01/26 00:05:30 INFO mapreduce.Job: Job job_local1386910411_0001 running in uber mode : false\n",
      "16/01/26 00:05:30 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/01/26 00:05:30 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/01/26 00:05:30 INFO mapred.MapTask: Finished spill 0\n",
      "16/01/26 00:05:30 INFO mapred.Task: Task:attempt_local1386910411_0001_m_000000_0 is done. And is in the process of committing\n",
      "16/01/26 00:05:30 INFO mapred.LocalJobRunner: Records R/W=10000/1\n",
      "16/01/26 00:05:30 INFO mapred.Task: Task 'attempt_local1386910411_0001_m_000000_0' done.\n",
      "16/01/26 00:05:30 INFO mapred.LocalJobRunner: Finishing task: attempt_local1386910411_0001_m_000000_0\n",
      "16/01/26 00:05:30 INFO mapred.LocalJobRunner: map task executor complete.\n",
      "16/01/26 00:05:30 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
      "16/01/26 00:05:30 INFO mapred.LocalJobRunner: Starting task: attempt_local1386910411_0001_r_000000_0\n",
      "16/01/26 00:05:30 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/01/26 00:05:30 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
      "16/01/26 00:05:30 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@23a83610\n",
      "16/01/26 00:05:30 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=353422528, maxSingleShuffleLimit=88355632, mergeThreshold=233258880, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "16/01/26 00:05:30 INFO reduce.EventFetcher: attempt_local1386910411_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "16/01/26 00:05:30 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1386910411_0001_m_000000_0 decomp: 108883 len: 108887 to MEMORY\n",
      "16/01/26 00:05:30 INFO reduce.InMemoryMapOutput: Read 108883 bytes from map-output for attempt_local1386910411_0001_m_000000_0\n",
      "16/01/26 00:05:30 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 108883, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->108883\n",
      "16/01/26 00:05:30 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "16/01/26 00:05:30 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/01/26 00:05:30 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "16/01/26 00:05:30 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/01/26 00:05:30 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 108873 bytes\n",
      "16/01/26 00:05:30 INFO reduce.MergeManagerImpl: Merged 1 segments, 108883 bytes to disk to satisfy reduce memory limit\n",
      "16/01/26 00:05:30 INFO reduce.MergeManagerImpl: Merging 1 files, 108887 bytes from disk\n",
      "16/01/26 00:05:30 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "16/01/26 00:05:30 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/01/26 00:05:30 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 108873 bytes\n",
      "16/01/26 00:05:30 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/01/26 00:05:30 INFO streaming.PipeMapRed: PipeMapRed exec [/root/hw2/mapreducer.py]\n",
      "16/01/26 00:05:30 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
      "16/01/26 00:05:30 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "16/01/26 00:05:30 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/26 00:05:30 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/26 00:05:30 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/26 00:05:30 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/26 00:05:30 INFO streaming.PipeMapRed: R/W/S=10000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/26 00:05:30 INFO streaming.PipeMapRed: Records R/W=10000/1\n",
      "16/01/26 00:05:30 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/01/26 00:05:30 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/01/26 00:05:30 INFO mapred.Task: Task:attempt_local1386910411_0001_r_000000_0 is done. And is in the process of committing\n",
      "16/01/26 00:05:30 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/01/26 00:05:30 INFO mapred.Task: Task attempt_local1386910411_0001_r_000000_0 is allowed to commit now\n",
      "16/01/26 00:05:30 INFO output.FileOutputCommitter: Saved output of task 'attempt_local1386910411_0001_r_000000_0' to hdfs://localhost:54310/user/root/wk2/hw21/output/_temporary/0/task_local1386910411_0001_r_000000\n",
      "16/01/26 00:05:30 INFO mapred.LocalJobRunner: Records R/W=10000/1 > reduce\n",
      "16/01/26 00:05:30 INFO mapred.Task: Task 'attempt_local1386910411_0001_r_000000_0' done.\n",
      "16/01/26 00:05:30 INFO mapred.LocalJobRunner: Finishing task: attempt_local1386910411_0001_r_000000_0\n",
      "16/01/26 00:05:30 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
      "16/01/26 00:05:31 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/01/26 00:05:31 INFO mapreduce.Job: Job job_local1386910411_0001 completed successfully\n",
      "16/01/26 00:05:31 INFO mapreduce.Job: Counters: 35\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=429654\n",
      "\t\tFILE: Number of bytes written=1099275\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=157762\n",
      "\t\tHDFS: Number of bytes written=88881\n",
      "\t\tHDFS: Number of read operations=13\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=4\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=10000\n",
      "\t\tMap output records=10000\n",
      "\t\tMap output bytes=88881\n",
      "\t\tMap output materialized bytes=108887\n",
      "\t\tInput split bytes=117\n",
      "\t\tCombine input records=10000\n",
      "\t\tCombine output records=10000\n",
      "\t\tReduce input groups=6295\n",
      "\t\tReduce shuffle bytes=108887\n",
      "\t\tReduce input records=10000\n",
      "\t\tReduce output records=10000\n",
      "\t\tSpilled Records=20000\n",
      "\t\tShuffled Maps =1\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=1\n",
      "\t\tGC time elapsed (ms)=32\n",
      "\t\tTotal committed heap usage (bytes)=1013448704\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=78881\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=88881\n",
      "16/01/26 00:05:31 INFO streaming.StreamJob: Output directory: /user/root/wk2/hw21/output\n"
     ]
    }
   ],
   "source": [
    "# Check whether the output folder already exists or not\n",
    "!hdfs dfs -rm -r /user/root/wk2/hw21/output\n",
    "\n",
    "# Run Hadoop job\n",
    "!hadoop jar hadoop-streaming-2.7.1.jar \\\n",
    "-D mapreduce.job.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \\\n",
    "-D mapreduce.partition.keycomparator.options=-nr \\\n",
    "-D mapred.combine.tasks=2 \\\n",
    "-mapper /root/hw2/mapreducer.py \\\n",
    "-combiner /root/hw2/mapreducer.py \\\n",
    "-reducer /root/hw2/mapreducer.py \\\n",
    "-input /user/root/wk2/hw21/input/randomrecords.txt \\\n",
    "-output /user/root/wk2/hw21/output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/26 00:14:21 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/01/26 00:14:22 WARN hdfs.DFSClient: DFSInputStream has been closed already\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -copyToLocal /user/root/wk2/hw21/output/part*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Display the 10 largest numbers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ls part-*\n",
      "---------\n",
      "part-00000\n",
      " \n",
      "Printing 10 largest numbers\n",
      "---------------------------\n",
      "9999\n",
      "9999\n",
      "9998\n",
      "9996\n",
      "9996\n",
      "9995\n",
      "9994\n",
      "9993\n",
      "9990\n",
      "9990\n",
      "\n",
      "\n",
      "Printing 10 smallest numbers\n",
      "----------------------------\n",
      "11\n",
      "10\n",
      "9\n",
      "9\n",
      "8\n",
      "7\n",
      "7\n",
      "5\n",
      "4\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "!echo \"ls part-*\"\n",
    "!echo \"---------\"\n",
    "!ls part*\n",
    "!echo \" \"\n",
    "!echo \"Printing 10 largest numbers\"\n",
    "!echo \"---------------------------\"\n",
    "!head -10 part-00000|cut -d \",\" -f 1\n",
    "!echo \"\"\n",
    "!echo \"\"\n",
    "!echo \"Printing 10 smallest numbers\"\n",
    "!echo \"----------------------------\"\n",
    "!tail -10 part-00000|cut -d \",\" -f 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW2.2. WORDCOUNT\n",
    "Using the Enron data from HW1 and Hadoop MapReduce streaming, write the mapper/reducer job that  will determine the word count (number of occurrences) of each white-space delimitted token (assume spaces, fullstops, comma as delimiters). Examine the word “assistance” and report its word count results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data validation and cleansing**\n",
    "By exploring the data in the input dataset, 2 problems with 3 records are identified:\n",
    "- There are 2 records with only 3 fields instead of 4 fields.\n",
    "- There is 1 record with extra new line character in the body field\n",
    "\n",
    "**Data cleansing algorithm**\n",
    "- Open enronemail_1new.txt with \"w\" permissions\n",
    "- Initialize prev_line = \"\"\n",
    "- For each line as line in enronemail_1h.txt file\n",
    "   - Tokenize with delimiter \"\\t\"\n",
    "   - If number of tokens >= 3 then\n",
    "   - If number of tokens == 3 then\n",
    "      - Add \"\\t\" as another token between 2nd and 3rd tokens. Now total number of tokens = 4.\n",
    "      - Update line by concatenating all the 4 okens\n",
    "      - If prev_line != \"\" then write prev_line in enronemail_1new.txt\n",
    "   - prev_line = line\n",
    "   - If number of tokens == 1 then\n",
    "      - prev_line = prev_line + line\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleanup completed\n"
     ]
    }
   ],
   "source": [
    "# Data cleansing algorithm\n",
    "\n",
    "import os\n",
    "import re\n",
    "\n",
    "# Open enronemail_1new.txt with \"w\" permissions.\n",
    "with open(\"enronemail_1new.txt\", \"w\") as new:\n",
    "    with open(\"enronemail_1h.txt\", \"rU\") as old:\n",
    "        # curr_line is the line to be written to new file. Initially it is set to \"\". \n",
    "        prev_line = \"\"\n",
    "        \n",
    "        # For every line in enronemail_1h.txt file\n",
    "        for line in old:\n",
    "            \n",
    "            line = line.strip()\n",
    "            # Split the line into tokens\n",
    "            tokens = line.split('\\t')\n",
    "            \n",
    "            if len(tokens) >= 3:\n",
    "                \n",
    "                # If subject field is missed out, add blank token and reconstruct the line\n",
    "                if len(tokens) == 3:\n",
    "                    line = tokens[0] + '\\t' + tokens[1] + '\\t' + '' + '\\t' + tokens[2]\n",
    "                \n",
    "                # If len(tokens) == 4 then this line is valid. Keep it in buffer. \n",
    "                # Now copy the previous line (if not blank). \n",
    "                if prev_line != \"\":\n",
    "                    prev_line += '\\n'\n",
    "                    new.write(prev_line)\n",
    "                prev_line = line\n",
    "            \n",
    "            # If there is only one field, it must be because of an \n",
    "            # extra new line character in the previous line body field.\n",
    "            if len(tokens) == 1:\n",
    "                # Add this line too to the previous line\n",
    "                prev_line += line\n",
    "        \n",
    "        # Add the last line to the new file\n",
    "        new.write(prev_line)\n",
    "\n",
    "# Now rename enronemail_1new.txt to enronemail_1h.txt\n",
    "os.rename('enronemail_1new.txt', 'enronemail_1h.txt')\n",
    "\n",
    "print \"Cleanup completed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "## mapper.py\n",
    "## Author: Prabhakar Gundugola\n",
    "## Description: mapper code for HW2.2\n",
    "\n",
    "import sys\n",
    "import re\n",
    "import string\n",
    "\n",
    "WORD_RE = re.compile(r\"[\\w']+\")\n",
    "\n",
    "## collect user input\n",
    "for line in sys.stdin:\n",
    "    tokens = line.lower().split('\\t')\n",
    "\n",
    "    # Concatenate subject and body fields and store it in word_string\n",
    "    word_string = tokens[2] + ' ' + tokens[3].strip()\n",
    "\n",
    "    # Remove punctuation\n",
    "    word_string = word_string.translate(string.maketrans(\"\",\"\"), \n",
    "                                        string.punctuation)\n",
    "\n",
    "    for word in WORD_RE.findall(word_string):\n",
    "        print('{0}\\t{1}'.format(word.lower(), 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "## reducer.py\n",
    "## Author: Prabhakar Gundugola\n",
    "## Description: reducer code for HW2.2\n",
    "\n",
    "import sys\n",
    "\n",
    "def wcount(prev_word, counts):\n",
    "    if prev_word is not None:\n",
    "        print prev_word + \"\\t\" + str(counts)\n",
    "\n",
    "prev_word = None\n",
    "counts = 0\n",
    "for line in sys.stdin:\n",
    "    word, value = line.split('\\t', 1)\n",
    "    if word != prev_word:\n",
    "        wcount(prev_word, counts)\n",
    "        prev_word = word\n",
    "        counts = 0\n",
    "\n",
    "    counts += eval(value)\n",
    "wcount(prev_word, counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Change permissions on mapper.py and reducer.py\n",
    "!chmod a+x mapper.py\n",
    "!chmod a+x reducer.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Testing the code with command line**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assistance\t10\r\n"
     ]
    }
   ],
   "source": [
    "!cat enronemail_1h.txt| ./mapper.py \"assistance\" | ./reducer.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the MapReduce in Hadoop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/26 01:11:00 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/01/26 01:11:01 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /user/root/wk2/hw22\n",
      "16/01/26 01:11:02 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/01/26 01:11:05 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# Ensure hw22 folder doesn't exist\n",
    "!hdfs dfs -rm -r /user/root/wk2/hw22\n",
    "\n",
    "# Create HDFS input folder\n",
    "!hdfs dfs -mkdir -p /user/root/wk2/hw22/input\n",
    "\n",
    "# Copy Enron email file to HDFS input folder\n",
    "!hdfs dfs -put enronemail_1h.txt /user/root/wk2/hw22/input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Run Hadoop Streaming job**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/26 01:16:15 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/01/26 01:16:16 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /user/root/wk2/hw22/output\n",
      "16/01/26 01:16:17 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/01/26 01:16:18 INFO Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id\n",
      "16/01/26 01:16:18 INFO jvm.JvmMetrics: Initializing JVM Metrics with processName=JobTracker, sessionId=\n",
      "16/01/26 01:16:18 INFO jvm.JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized\n",
      "16/01/26 01:16:19 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/01/26 01:16:19 INFO mapreduce.JobSubmitter: number of splits:1\n",
      "16/01/26 01:16:19 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local126404396_0001\n",
      "16/01/26 01:16:19 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
      "16/01/26 01:16:19 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
      "16/01/26 01:16:19 INFO mapreduce.Job: Running job: job_local126404396_0001\n",
      "16/01/26 01:16:19 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
      "16/01/26 01:16:19 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/01/26 01:16:19 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
      "16/01/26 01:16:19 INFO mapred.LocalJobRunner: Starting task: attempt_local126404396_0001_m_000000_0\n",
      "16/01/26 01:16:19 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/01/26 01:16:19 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
      "16/01/26 01:16:19 INFO mapred.MapTask: Processing split: hdfs://localhost:54310/user/root/wk2/hw22/input/enronemail_1h.txt:0+203954\n",
      "16/01/26 01:16:19 INFO mapred.MapTask: numReduceTasks: 1\n",
      "16/01/26 01:16:19 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "16/01/26 01:16:19 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "16/01/26 01:16:19 INFO mapred.MapTask: soft limit at 83886080\n",
      "16/01/26 01:16:19 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "16/01/26 01:16:19 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "16/01/26 01:16:19 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "16/01/26 01:16:19 INFO streaming.PipeMapRed: PipeMapRed exec [/root/hw2/./mapper.py]\n",
      "16/01/26 01:16:19 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
      "16/01/26 01:16:19 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
      "16/01/26 01:16:19 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
      "16/01/26 01:16:19 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
      "16/01/26 01:16:19 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
      "16/01/26 01:16:19 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
      "16/01/26 01:16:19 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
      "16/01/26 01:16:19 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
      "16/01/26 01:16:19 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
      "16/01/26 01:16:19 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
      "16/01/26 01:16:19 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
      "16/01/26 01:16:19 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
      "16/01/26 01:16:19 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/26 01:16:19 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/26 01:16:20 INFO streaming.PipeMapRed: Records R/W=72/1\n",
      "16/01/26 01:16:20 INFO streaming.PipeMapRed: R/W/S=100/1035/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/26 01:16:20 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/01/26 01:16:20 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/01/26 01:16:20 INFO mapred.LocalJobRunner: \n",
      "16/01/26 01:16:20 INFO mapred.MapTask: Starting flush of map output\n",
      "16/01/26 01:16:20 INFO mapred.MapTask: Spilling map output\n",
      "16/01/26 01:16:20 INFO mapred.MapTask: bufstart = 0; bufend = 246970; bufvoid = 104857600\n",
      "16/01/26 01:16:20 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26088536(104354144); length = 125861/6553600\n",
      "16/01/26 01:16:20 INFO mapred.MapTask: Finished spill 0\n",
      "16/01/26 01:16:20 INFO mapred.Task: Task:attempt_local126404396_0001_m_000000_0 is done. And is in the process of committing\n",
      "16/01/26 01:16:20 INFO mapred.LocalJobRunner: Records R/W=72/1\n",
      "16/01/26 01:16:20 INFO mapred.Task: Task 'attempt_local126404396_0001_m_000000_0' done.\n",
      "16/01/26 01:16:20 INFO mapred.LocalJobRunner: Finishing task: attempt_local126404396_0001_m_000000_0\n",
      "16/01/26 01:16:20 INFO mapred.LocalJobRunner: map task executor complete.\n",
      "16/01/26 01:16:20 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
      "16/01/26 01:16:20 INFO mapred.LocalJobRunner: Starting task: attempt_local126404396_0001_r_000000_0\n",
      "16/01/26 01:16:20 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/01/26 01:16:20 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
      "16/01/26 01:16:20 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@74327a4\n",
      "16/01/26 01:16:20 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=353422528, maxSingleShuffleLimit=88355632, mergeThreshold=233258880, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "16/01/26 01:16:20 INFO reduce.EventFetcher: attempt_local126404396_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "16/01/26 01:16:20 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local126404396_0001_m_000000_0 decomp: 309904 len: 309908 to MEMORY\n",
      "16/01/26 01:16:20 INFO mapreduce.Job: Job job_local126404396_0001 running in uber mode : false\n",
      "16/01/26 01:16:20 INFO reduce.InMemoryMapOutput: Read 309904 bytes from map-output for attempt_local126404396_0001_m_000000_0\n",
      "16/01/26 01:16:20 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 309904, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->309904\n",
      "16/01/26 01:16:20 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/01/26 01:16:20 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "16/01/26 01:16:20 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/01/26 01:16:20 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "16/01/26 01:16:20 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/01/26 01:16:20 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 309900 bytes\n",
      "16/01/26 01:16:20 INFO reduce.MergeManagerImpl: Merged 1 segments, 309904 bytes to disk to satisfy reduce memory limit\n",
      "16/01/26 01:16:20 INFO reduce.MergeManagerImpl: Merging 1 files, 309908 bytes from disk\n",
      "16/01/26 01:16:20 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "16/01/26 01:16:20 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/01/26 01:16:20 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 309900 bytes\n",
      "16/01/26 01:16:20 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/01/26 01:16:20 INFO streaming.PipeMapRed: PipeMapRed exec [/root/hw2/./reducer.py]\n",
      "16/01/26 01:16:20 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
      "16/01/26 01:16:20 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "16/01/26 01:16:20 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/26 01:16:20 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/26 01:16:20 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/26 01:16:20 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/26 01:16:20 INFO streaming.PipeMapRed: R/W/S=10000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/26 01:16:21 INFO streaming.PipeMapRed: Records R/W=16560/1\n",
      "16/01/26 01:16:21 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/01/26 01:16:21 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/01/26 01:16:21 INFO mapred.Task: Task:attempt_local126404396_0001_r_000000_0 is done. And is in the process of committing\n",
      "16/01/26 01:16:21 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/01/26 01:16:21 INFO mapred.Task: Task attempt_local126404396_0001_r_000000_0 is allowed to commit now\n",
      "16/01/26 01:16:21 INFO output.FileOutputCommitter: Saved output of task 'attempt_local126404396_0001_r_000000_0' to hdfs://localhost:54310/user/root/wk2/hw22/output/_temporary/0/task_local126404396_0001_r_000000\n",
      "16/01/26 01:16:21 INFO mapred.LocalJobRunner: Records R/W=16560/1 > reduce\n",
      "16/01/26 01:16:21 INFO mapred.Task: Task 'attempt_local126404396_0001_r_000000_0' done.\n",
      "16/01/26 01:16:21 INFO mapred.LocalJobRunner: Finishing task: attempt_local126404396_0001_r_000000_0\n",
      "16/01/26 01:16:21 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
      "16/01/26 01:16:21 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/01/26 01:16:21 INFO mapreduce.Job: Job job_local126404396_0001 completed successfully\n",
      "16/01/26 01:16:21 INFO mapreduce.Job: Counters: 35\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=831696\n",
      "\t\tFILE: Number of bytes written=1695196\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=407908\n",
      "\t\tHDFS: Number of bytes written=57078\n",
      "\t\tHDFS: Number of read operations=13\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=4\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=100\n",
      "\t\tMap output records=31466\n",
      "\t\tMap output bytes=246970\n",
      "\t\tMap output materialized bytes=309908\n",
      "\t\tInput split bytes=117\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=5740\n",
      "\t\tReduce shuffle bytes=309908\n",
      "\t\tReduce input records=31466\n",
      "\t\tReduce output records=5740\n",
      "\t\tSpilled Records=62932\n",
      "\t\tShuffled Maps =1\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=1\n",
      "\t\tGC time elapsed (ms)=25\n",
      "\t\tTotal committed heap usage (bytes)=1013448704\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=203954\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=57078\n",
      "16/01/26 01:16:21 INFO streaming.StreamJob: Output directory: /user/root/wk2/hw22/output\n"
     ]
    }
   ],
   "source": [
    "#Delete output folder if exists\n",
    "!hdfs dfs -rm -r /user/root/wk2/hw22/output\n",
    "\n",
    "# Run Hadoop Streaming job\n",
    "!hadoop jar hadoop-streaming-2.7.1.jar \\\n",
    "-mapper mapper.py \\\n",
    "-reducer reducer.py \\\n",
    "-input /user/root/wk2/hw22/input \\\n",
    "-output /user/root/wk2/hw22/output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/26 01:16:30 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/01/26 01:16:31 WARN hdfs.DFSClient: DFSInputStream has been closed already\n"
     ]
    }
   ],
   "source": [
    "!rm part*\n",
    "!hdfs dfs -copyToLocal /user/root/wk2/hw22/output/part*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Examine the word \"assistance\"**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assistance\t10\r\n"
     ]
    }
   ],
   "source": [
    "!grep assistance part*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW2.2.1. Using Hadoop MapReduce and your wordcount job (from HW2.2) determine the top-10 occurring tokens (most frequent tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "## mapper.py\n",
    "## Author: Prabhakar Gundugola\n",
    "## Description: mapper code for HW2.2.1\n",
    "import sys\n",
    "\n",
    "#Swap key and value\n",
    "for line in sys.stdin:\n",
    "    vals = line.strip().split('\\t')\n",
    "    print('{0}\\t{1}'.format(vals[1], vals[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%writefile reduce.py\n",
    "#!/usr/bin/python\n",
    "## mapper.py\n",
    "## Author: Prabhakar Gundugola\n",
    "## Description: reducer code for HW2.2.1\n",
    "import sys\n",
    "\n",
    "#Swap key and value\n",
    "for line in sys.stdin:\n",
    "    vals = line.replace('\\n', '').split('\\t')\n",
    "    print('{0}\\t{1}'.format(vals[1], vals[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/26 01:34:12 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/01/26 01:34:13 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /user/root/wk2/hw22/output_1\n",
      "16/01/26 01:34:14 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/01/26 01:34:15 INFO Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id\n",
      "16/01/26 01:34:15 INFO jvm.JvmMetrics: Initializing JVM Metrics with processName=JobTracker, sessionId=\n",
      "16/01/26 01:34:15 INFO jvm.JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized\n",
      "16/01/26 01:34:15 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/01/26 01:34:15 INFO mapreduce.JobSubmitter: number of splits:1\n",
      "16/01/26 01:34:16 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local2107275415_0001\n",
      "16/01/26 01:34:16 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
      "16/01/26 01:34:16 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
      "16/01/26 01:34:16 INFO mapreduce.Job: Running job: job_local2107275415_0001\n",
      "16/01/26 01:34:16 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
      "16/01/26 01:34:16 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/01/26 01:34:16 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
      "16/01/26 01:34:16 INFO mapred.LocalJobRunner: Starting task: attempt_local2107275415_0001_m_000000_0\n",
      "16/01/26 01:34:16 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/01/26 01:34:16 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
      "16/01/26 01:34:16 INFO mapred.MapTask: Processing split: hdfs://localhost:54310/user/root/wk2/hw22/output/part-00000:0+57078\n",
      "16/01/26 01:34:16 INFO mapred.MapTask: numReduceTasks: 1\n",
      "16/01/26 01:34:16 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "16/01/26 01:34:16 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "16/01/26 01:34:16 INFO mapred.MapTask: soft limit at 83886080\n",
      "16/01/26 01:34:16 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "16/01/26 01:34:16 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "16/01/26 01:34:16 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "16/01/26 01:34:16 INFO streaming.PipeMapRed: PipeMapRed exec [/root/hw2/./mapreduce.py]\n",
      "16/01/26 01:34:16 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
      "16/01/26 01:34:16 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
      "16/01/26 01:34:16 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
      "16/01/26 01:34:16 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
      "16/01/26 01:34:16 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
      "16/01/26 01:34:16 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
      "16/01/26 01:34:16 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
      "16/01/26 01:34:16 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
      "16/01/26 01:34:16 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
      "16/01/26 01:34:16 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
      "16/01/26 01:34:16 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
      "16/01/26 01:34:16 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
      "16/01/26 01:34:16 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/26 01:34:16 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/26 01:34:16 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/26 01:34:16 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/26 01:34:16 INFO streaming.PipeMapRed: Records R/W=5740/1\n",
      "16/01/26 01:34:16 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/01/26 01:34:17 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/01/26 01:34:17 INFO mapred.LocalJobRunner: \n",
      "16/01/26 01:34:17 INFO mapred.MapTask: Starting flush of map output\n",
      "16/01/26 01:34:17 INFO mapred.MapTask: Spilling map output\n",
      "16/01/26 01:34:17 INFO mapred.MapTask: bufstart = 0; bufend = 57078; bufvoid = 104857600\n",
      "16/01/26 01:34:17 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26191440(104765760); length = 22957/6553600\n",
      "16/01/26 01:34:17 INFO mapred.MapTask: Finished spill 0\n",
      "16/01/26 01:34:17 INFO mapred.Task: Task:attempt_local2107275415_0001_m_000000_0 is done. And is in the process of committing\n",
      "16/01/26 01:34:17 INFO mapred.LocalJobRunner: Records R/W=5740/1\n",
      "16/01/26 01:34:17 INFO mapred.Task: Task 'attempt_local2107275415_0001_m_000000_0' done.\n",
      "16/01/26 01:34:17 INFO mapred.LocalJobRunner: Finishing task: attempt_local2107275415_0001_m_000000_0\n",
      "16/01/26 01:34:17 INFO mapred.LocalJobRunner: map task executor complete.\n",
      "16/01/26 01:34:17 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
      "16/01/26 01:34:17 INFO mapred.LocalJobRunner: Starting task: attempt_local2107275415_0001_r_000000_0\n",
      "16/01/26 01:34:17 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/01/26 01:34:17 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
      "16/01/26 01:34:17 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@39741f43\n",
      "16/01/26 01:34:17 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=353422528, maxSingleShuffleLimit=88355632, mergeThreshold=233258880, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "16/01/26 01:34:17 INFO reduce.EventFetcher: attempt_local2107275415_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "16/01/26 01:34:17 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local2107275415_0001_m_000000_0 decomp: 68560 len: 68564 to MEMORY\n",
      "16/01/26 01:34:17 INFO reduce.InMemoryMapOutput: Read 68560 bytes from map-output for attempt_local2107275415_0001_m_000000_0\n",
      "16/01/26 01:34:17 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 68560, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->68560\n",
      "16/01/26 01:34:17 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "16/01/26 01:34:17 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/01/26 01:34:17 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "16/01/26 01:34:17 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/01/26 01:34:17 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 68556 bytes\n",
      "16/01/26 01:34:17 INFO reduce.MergeManagerImpl: Merged 1 segments, 68560 bytes to disk to satisfy reduce memory limit\n",
      "16/01/26 01:34:17 INFO reduce.MergeManagerImpl: Merging 1 files, 68564 bytes from disk\n",
      "16/01/26 01:34:17 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "16/01/26 01:34:17 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/01/26 01:34:17 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 68556 bytes\n",
      "16/01/26 01:34:17 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/01/26 01:34:17 INFO mapreduce.Job: Job job_local2107275415_0001 running in uber mode : false\n",
      "16/01/26 01:34:17 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/01/26 01:34:17 INFO mapred.Task: Task:attempt_local2107275415_0001_r_000000_0 is done. And is in the process of committing\n",
      "16/01/26 01:34:17 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/01/26 01:34:17 INFO mapred.Task: Task attempt_local2107275415_0001_r_000000_0 is allowed to commit now\n",
      "16/01/26 01:34:17 INFO output.FileOutputCommitter: Saved output of task 'attempt_local2107275415_0001_r_000000_0' to hdfs://localhost:54310/user/root/wk2/hw22/output_1/_temporary/0/task_local2107275415_0001_r_000000\n",
      "16/01/26 01:34:17 INFO mapred.LocalJobRunner: reduce > reduce\n",
      "16/01/26 01:34:17 INFO mapred.Task: Task 'attempt_local2107275415_0001_r_000000_0' done.\n",
      "16/01/26 01:34:17 INFO mapred.LocalJobRunner: Finishing task: attempt_local2107275415_0001_r_000000_0\n",
      "16/01/26 01:34:17 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
      "16/01/26 01:34:18 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/01/26 01:34:18 INFO mapreduce.Job: Job job_local2107275415_0001 completed successfully\n",
      "16/01/26 01:34:18 INFO mapreduce.Job: Counters: 35\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=348994\n",
      "\t\tFILE: Number of bytes written=973680\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=114156\n",
      "\t\tHDFS: Number of bytes written=57078\n",
      "\t\tHDFS: Number of read operations=13\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=4\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=5740\n",
      "\t\tMap output records=5740\n",
      "\t\tMap output bytes=57078\n",
      "\t\tMap output materialized bytes=68564\n",
      "\t\tInput split bytes=111\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=107\n",
      "\t\tReduce shuffle bytes=68564\n",
      "\t\tReduce input records=5740\n",
      "\t\tReduce output records=5740\n",
      "\t\tSpilled Records=11480\n",
      "\t\tShuffled Maps =1\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=1\n",
      "\t\tGC time elapsed (ms)=37\n",
      "\t\tTotal committed heap usage (bytes)=1013448704\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=57078\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=57078\n",
      "16/01/26 01:34:18 INFO streaming.StreamJob: Output directory: /user/root/wk2/hw22/output_1\n"
     ]
    }
   ],
   "source": [
    "# Change permissions on mapper\n",
    "!chmod a+x mapreduce.py\n",
    "\n",
    "# Delete output folder if exists\n",
    "!hdfs dfs -rm -r /user/root/wk2/hw22/output_1\n",
    "\n",
    "# Run Hadoop Streaming job\n",
    "!hadoop jar hadoop-streaming-2.7.1.jar \\\n",
    "-D mapreduce.job.output.key.comparator.class=org.apache.hadoop.mapreduce.lib.partition.KeyFieldBasedComparator \\\n",
    "-D mapreduce.partition.keycomparator.options=-k1,1n \\\n",
    "-mapper mapreduce.py \\\n",
    "-reducer org.apache.hadoop.mapred.lib.IdentityReducer \\\n",
    "-input /user/root/wk2/hw22/output \\\n",
    "-output /user/root/wk2/hw22/output_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/26 01:34:25 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/01/26 01:34:26 WARN hdfs.DFSClient: DFSInputStream has been closed already\n"
     ]
    }
   ],
   "source": [
    "!rm part*\n",
    "!hdfs dfs -copyToLocal /user/root/wk2/hw22/output_1/part*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Printing top 10 occurring tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1246\tthe\r\n",
      "961\tto\r\n",
      "661\tand\r\n",
      "560\tof\r\n",
      "529\ta\r\n",
      "427\tyou\r\n",
      "415\tin\r\n",
      "391\tyour\r\n",
      "373\tfor\r\n",
      "260\tthis\r\n"
     ]
    }
   ],
   "source": [
    "!tail -10 part-00000 |sort -nrk1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW2.3. Multinomial NAIVE BAYES with NO Smoothing\n",
    "Using the Enron data from HW1 and Hadoop MapReduce, write  a mapper/reducer job(s) that\n",
    "   will both learn  Naive Bayes classifier and classify the Enron email messages using the learnt Naive Bayes classifier. Use all white-space delimitted tokens as independent input variables (assume spaces, fullstops, commas as delimiters). Note: for multinomial Naive Bayes, the $$Pr(X=“assistance”|Y=SPAM)$$ is calculated as follows:\n",
    "\n",
    "   $$ \\frac{\\mbox{the number of times “assistance” occurs in SPAM labeled documents}}{\\mbox{the number of words in documents labeled SPAM}}$$\n",
    "\n",
    "   E.g.,   “assistance” occurs 5 times in all of the documents Labeled SPAM, and the length in terms of the number of words in all documents labeled as SPAM (when concatenated) is 1,000. Then Pr(X=“assistance”|Y=SPAM) = 5/1000. Note this is a multinomial estimation of the class conditional for a Naive Bayes Classifier. No smoothing is needed in this HW. Multiplying lots of probabilities, which are between 0 and 1, can result in floating-point underflow. Since log(xy) = log(x) + log(y), it is better to perform all computations by summing logs of probabilities rather than multiplying probabilities. Please pay attention to probabilites that are zero! They will need special attention. Count up how many times you need to process a zero probabilty for each class and report. \n",
    "\n",
    "   Report the performance of your learnt classifier in terms of misclassifcation error rate of your multinomial Naive Bayes Classifier. Plot a histogram of the  posterior probabilities (i.e., Pr(Class|Doc)) for each class over the training set. Summarize what you see. \n",
    "\n",
    "   Error Rate = misclassification rate with respect to a provided set (say training set in this case). It is more formally defined here:\n",
    "\n",
    "Let DF represent the evalution set in the following:\n",
    "Err(Model, DF) = |{(X, c(X)) ? DF : c(X) != Model(x)}|   / |DF|\n",
    "\n",
    "Where || denotes set cardinality; c(X) denotes the class of the tuple X in DF; and Model(X) denotes the class inferred by the Model “Model”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper1.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper1.py\n",
    "#!/usr/bin/python\n",
    "## mapper231.py\n",
    "## Author: Prabhakar Gundugola\n",
    "## Description: mapper-1 code for HW2.3\n",
    "\n",
    "import sys\n",
    "import re\n",
    "import string\n",
    "\n",
    "WORD_RE = re.compile(r\"[\\w']+\") # Compile regex to easily parse complete words\n",
    "\n",
    "spam_email_count = 0\n",
    "ham_email_count = 0\n",
    "\n",
    "total_ham_count = 0\n",
    "total_spam_count = 0\n",
    "\n",
    "for line in sys.stdin:\n",
    "    \n",
    "    # Tokenize each line. Line format - DOC_ID <tab> SPAM <tab> subject <tab> body\n",
    "    tokens = line.lower().strip().split('\\t')\n",
    "    \n",
    "    spam = tokens[1]\n",
    "    if spam == '1':\n",
    "        spam_email_count += 1\n",
    "    else:\n",
    "        ham_email_count += 1\n",
    "    \n",
    "    # Concatenate subject and body fields and store it in word_string\n",
    "    word_string = tokens[2].strip() + ' ' + tokens[3].strip()\n",
    "    \n",
    "    # Remove punctuation\n",
    "    word_string = word_string.translate(string.maketrans(\"\", \"\"), \n",
    "                                       string.punctuation)\n",
    "    \n",
    "    word_list = WORD_RE.findall(word_string)\n",
    "    # Get unique words\n",
    "    unique_words = set(word_list)\n",
    "    \n",
    "    for word in unique_words:\n",
    "        word_freq = word_list.count(word)\n",
    "        print('{0}\\t{1}\\t{2}'.format(word, spam, str(word_freq)))\n",
    "\n",
    "print('{0}\\t{1}\\t{2}'.format('0000000DOC_CLASS', \n",
    "                                   str(spam_email_count), \n",
    "                                   str(ham_email_count)))          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer1.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer1.py\n",
    "#!/usr/bin/python\n",
    "## reducer231.py\n",
    "## Author: Prabhakar Gundugola\n",
    "## Description: reducer-1 code for HW2.3\n",
    "\n",
    "import sys\n",
    "import math\n",
    "\n",
    "spam_email_count = 0\n",
    "ham_email_count = 0\n",
    "\n",
    "spam_word_count = 0\n",
    "ham_word_count = 0\n",
    "\n",
    "total_spam_count = 0\n",
    "total_ham_count = 0\n",
    "\n",
    "total_docs = 0\n",
    "\n",
    "spam_words_freq = {}\n",
    "ham_words_freq = {}\n",
    "\n",
    "priors_calc = 0\n",
    "\n",
    "for line in sys.stdin:\n",
    "    tokens = line.strip().split('\\t')\n",
    "    \n",
    "    if tokens[0] == \"0000000DOC_CLASS\":\n",
    "        spam_email_count = int(tokens[1])\n",
    "        ham_email_count = int(tokens[2])\n",
    "        continue\n",
    "    \n",
    "    try:\n",
    "        count = int(tokens[2])\n",
    "        spam = int(tokens[1])\n",
    "    except ValueError:\n",
    "        continue\n",
    "   \n",
    "    \n",
    "    word, spam, frequency = tokens[0], spam, count\n",
    "    if spam == 1:\n",
    "        total_spam_count += frequency\n",
    "        if word in spam_words_freq:\n",
    "            spam_words_freq[word] += frequency\n",
    "        else:\n",
    "            spam_words_freq[word] = frequency\n",
    "        \n",
    "        if word not in ham_words_freq:\n",
    "            ham_words_freq[word] = 0\n",
    "    else:\n",
    "        total_ham_count += frequency\n",
    "        if word in ham_words_freq:\n",
    "            ham_words_freq[word] += frequency\n",
    "        else:\n",
    "            ham_words_freq[word] = frequency\n",
    "        \n",
    "        if word not in spam_words_freq:\n",
    "            spam_words_freq[word] = 0\n",
    "            \n",
    "prior_spam = math.log(1.0*spam_email_count/(spam_email_count + ham_email_count))\n",
    "prior_ham = math.log(1.0*ham_email_count/(spam_email_count + ham_email_count))   \n",
    "print('{0}\\t{1}\\t{2}'.format(\"0000000PRIORS\", str(prior_spam), str(prior_ham)))\n",
    "\n",
    "\n",
    "# Calculate conditional probability\n",
    "prob_spam_words = {}\n",
    "prob_ham_words = {}\n",
    "\n",
    "for word in ham_words_freq:\n",
    "    if spam_words_freq[word] > 0:\n",
    "        prob_spam_words[word] = math.log((1.0)*(spam_words_freq[word])/total_spam_count)\n",
    "    else:\n",
    "        prob_spam_words[word] = 0\n",
    "    if ham_words_freq[word] > 0:\n",
    "        prob_ham_words[word] = math.log((1.0)*(ham_words_freq[word])/total_ham_count)\n",
    "    else:\n",
    "        prob_ham_words[word] = 0\n",
    "    \n",
    "    print('{0}\\t{1}\\t{2}'.format(word, str(prob_spam_words[word]), str(prob_ham_words[word])))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper2.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper2.py\n",
    "#!/usr/bin/python\n",
    "## mapper-2.py\n",
    "## Author: Prabhakar Gundugola\n",
    "## Description: mapper-2 code for HW2.3\n",
    "\n",
    "import sys\n",
    "from math import log, exp\n",
    "import re\n",
    "import string\n",
    "\n",
    "WORD_RE = re.compile(r\"[\\w']+\")\n",
    "\n",
    "words = {}\n",
    "zero_probs_spam = 0\n",
    "zero_probs_ham = 0\n",
    "\n",
    "with open('hw23out.txt', 'rb') as fp:\n",
    "    for line in fp.readlines():\n",
    "        tokens = line.strip().split('\\t')\n",
    "        if tokens[0] == \"0000000PRIORS\":\n",
    "            prior_spam, prior_ham = float(tokens[1]), float(tokens[2])\n",
    "            continue\n",
    "        \n",
    "        words[tokens[0]] = {'p_spam':float(tokens[1]), 'p_ham':float(tokens[2])}\n",
    "\n",
    "count = 0        \n",
    "for line in sys.stdin:\n",
    "    tokens = line.strip().split('\\t')\n",
    "    count += 1\n",
    "    \n",
    "    # Concatenate subject and body fields and store it in word_string\n",
    "    word_string = tokens[2] + ' ' + tokens[3].strip()\n",
    "    \n",
    "    # Remove punctuation\n",
    "    word_string = word_string.translate(string.maketrans(\"\", \"\"), \n",
    "                                       string.punctuation)\n",
    "    \n",
    "    word_list = WORD_RE.findall(word_string)\n",
    "    \n",
    "    prior_spam_doc, prior_ham_doc = prior_spam, prior_ham\n",
    "    \n",
    "    for word in word_list:\n",
    "        word = word.lower().strip()\n",
    "        try:\n",
    "            if words[word]['p_spam'] <> 0:\n",
    "                prior_spam_doc += words[word]['p_spam']\n",
    "            else:\n",
    "                prior_spam_doc += float('-inf')\n",
    "                zero_probs_spam += 1\n",
    "        \n",
    "            if words[word]['p_ham'] <> 0:\n",
    "                prior_ham_doc += words[word]['p_ham']\n",
    "            \n",
    "            else:\n",
    "                prior_ham_doc += float('-inf')\n",
    "                zero_probs_ham += 1\n",
    "        except:\n",
    "            continue\n",
    "            \n",
    "        \n",
    "    predicted = 0\n",
    "    if prior_spam_doc == float('-inf'):\n",
    "        predicted = 0\n",
    "        prior_spam_doc = 0\n",
    "    elif prior_ham_doc == float('-inf'):\n",
    "        predicted = 1\n",
    "        prior_ham_doc = 0\n",
    "    elif prior_spam_doc > prior_ham_doc:\n",
    "        predicted = 1\n",
    "\n",
    "    values = tokens[1] + '\\t' + str(predicted) + '\\t' + str(prior_spam_doc) \n",
    "    values += '\\t' + str(prior_ham_doc) + '\\t' + str(zero_probs_spam)\n",
    "    values += '\\t' + str(zero_probs_ham)\n",
    "    print tokens[0] + '\\t' + values\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer2.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer2.py\n",
    "#!/usr/bin/python\n",
    "## reducer-2.py\n",
    "## Author: Prabhakar Gundugola\n",
    "## Description: reducer-2 code for HW2.3\n",
    "\n",
    "import sys\n",
    "\n",
    "count = 0\n",
    "correct = 0\n",
    "wrong = 0\n",
    "zero_probs_spam = 0\n",
    "zero_probs_ham = 0\n",
    "\n",
    "for line in sys.stdin:\n",
    "    tokens = line.strip().split('\\t')\n",
    "    spam, predicted = int(tokens[1]), int(tokens[2])\n",
    "    count += 1\n",
    "    zero_probs_spam += int(tokens[5])\n",
    "    zero_probs_ham += int(tokens[6])\n",
    "    if spam == predicted:\n",
    "        correct += 1\n",
    "    else:\n",
    "        wrong += 1\n",
    "    \n",
    "    print line\n",
    "\n",
    "training_error = 100.0*wrong/count\n",
    "accuracy = 100.0*correct/count\n",
    "\n",
    "print 'Training error: {0}%'.format(training_error)\n",
    "print 'Accuracy: {0}%'.format(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x mapper1.py\n",
    "!chmod a+x mapper2.py\n",
    "!chmod a+x reducer1.py\n",
    "!chmod a+x reducer2.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Command line testing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!cat enronemail_1h.txt|./mapper1.py|sort|./reducer1.py > hw23out.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training error: 0.0%\r\n",
      "Accuracy: 100.0%\r\n"
     ]
    }
   ],
   "source": [
    "!cat enronemail_1h.txt|./mapper2.py|sort|./reducer2.py "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!cat enronemail_1h.txt|./mapper2.py|sort|./reducer2.py > histogram.txt\n",
    "!grep assistance histogram.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Running the Hadoop Streaming for the first MapReduce job**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/26 08:27:48 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "rm: `/user/root/wk2/hw23': No such file or directory\n",
      "16/01/26 08:27:51 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/01/26 08:27:54 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# Ensure the output folder doesn't exist\n",
    "!hdfs dfs -rm -r /user/root/wk2/hw23\n",
    "!hdfs dfs -mkdir -p /user/root/wk2/hw23/input\n",
    "\n",
    "!hdfs dfs -put enronemail_1h.txt /user/root/wk2/hw23/input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/26 08:28:44 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "rm: `/user/root/wk2/hw23/output_1': No such file or directory\n",
      "16/01/26 08:28:46 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/01/26 08:28:47 INFO Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id\n",
      "16/01/26 08:28:47 INFO jvm.JvmMetrics: Initializing JVM Metrics with processName=JobTracker, sessionId=\n",
      "16/01/26 08:28:47 INFO jvm.JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized\n",
      "16/01/26 08:28:48 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/01/26 08:28:48 INFO mapreduce.JobSubmitter: number of splits:1\n",
      "16/01/26 08:28:48 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local998646625_0001\n",
      "16/01/26 08:28:48 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
      "16/01/26 08:28:48 INFO mapreduce.Job: Running job: job_local998646625_0001\n",
      "16/01/26 08:28:48 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
      "16/01/26 08:28:48 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
      "16/01/26 08:28:48 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/01/26 08:28:48 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
      "16/01/26 08:28:48 INFO mapred.LocalJobRunner: Starting task: attempt_local998646625_0001_m_000000_0\n",
      "16/01/26 08:28:48 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/01/26 08:28:48 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
      "16/01/26 08:28:48 INFO mapred.MapTask: Processing split: hdfs://localhost:54310/user/root/wk2/hw23/input/enronemail_1h.txt:0+203954\n",
      "16/01/26 08:28:48 INFO mapred.MapTask: numReduceTasks: 1\n",
      "16/01/26 08:28:49 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "16/01/26 08:28:49 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "16/01/26 08:28:49 INFO mapred.MapTask: soft limit at 83886080\n",
      "16/01/26 08:28:49 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "16/01/26 08:28:49 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "16/01/26 08:28:49 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "16/01/26 08:28:49 INFO streaming.PipeMapRed: PipeMapRed exec [/root/hw2/./mapper1.py]\n",
      "16/01/26 08:28:49 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
      "16/01/26 08:28:49 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
      "16/01/26 08:28:49 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
      "16/01/26 08:28:49 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
      "16/01/26 08:28:49 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
      "16/01/26 08:28:49 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
      "16/01/26 08:28:49 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
      "16/01/26 08:28:49 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
      "16/01/26 08:28:49 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
      "16/01/26 08:28:49 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
      "16/01/26 08:28:49 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
      "16/01/26 08:28:49 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
      "16/01/26 08:28:49 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/26 08:28:49 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/26 08:28:49 INFO streaming.PipeMapRed: Records R/W=72/1\n",
      "16/01/26 08:28:49 INFO streaming.PipeMapRed: R/W/S=100/1999/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/26 08:28:49 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/01/26 08:28:49 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/01/26 08:28:49 INFO mapred.LocalJobRunner: \n",
      "16/01/26 08:28:49 INFO mapred.MapTask: Starting flush of map output\n",
      "16/01/26 08:28:49 INFO mapred.MapTask: Spilling map output\n",
      "16/01/26 08:28:49 INFO mapred.MapTask: bufstart = 0; bufend = 164166; bufvoid = 104857600\n",
      "16/01/26 08:28:49 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26153056(104612224); length = 61341/6553600\n",
      "16/01/26 08:28:49 INFO mapred.MapTask: Finished spill 0\n",
      "16/01/26 08:28:49 INFO mapred.Task: Task:attempt_local998646625_0001_m_000000_0 is done. And is in the process of committing\n",
      "16/01/26 08:28:49 INFO mapred.LocalJobRunner: Records R/W=72/1\n",
      "16/01/26 08:28:49 INFO mapred.Task: Task 'attempt_local998646625_0001_m_000000_0' done.\n",
      "16/01/26 08:28:49 INFO mapred.LocalJobRunner: Finishing task: attempt_local998646625_0001_m_000000_0\n",
      "16/01/26 08:28:49 INFO mapred.LocalJobRunner: map task executor complete.\n",
      "16/01/26 08:28:49 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
      "16/01/26 08:28:49 INFO mapred.LocalJobRunner: Starting task: attempt_local998646625_0001_r_000000_0\n",
      "16/01/26 08:28:49 INFO mapreduce.Job: Job job_local998646625_0001 running in uber mode : false\n",
      "16/01/26 08:28:49 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/01/26 08:28:49 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/01/26 08:28:49 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
      "16/01/26 08:28:49 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@7ddaca36\n",
      "16/01/26 08:28:49 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=353422528, maxSingleShuffleLimit=88355632, mergeThreshold=233258880, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "16/01/26 08:28:49 INFO reduce.EventFetcher: attempt_local998646625_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "16/01/26 08:28:49 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local998646625_0001_m_000000_0 decomp: 194840 len: 194844 to MEMORY\n",
      "16/01/26 08:28:49 INFO reduce.InMemoryMapOutput: Read 194840 bytes from map-output for attempt_local998646625_0001_m_000000_0\n",
      "16/01/26 08:28:49 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 194840, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->194840\n",
      "16/01/26 08:28:49 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "16/01/26 08:28:49 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/01/26 08:28:49 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "16/01/26 08:28:49 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/01/26 08:28:49 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 194836 bytes\n",
      "16/01/26 08:28:50 INFO reduce.MergeManagerImpl: Merged 1 segments, 194840 bytes to disk to satisfy reduce memory limit\n",
      "16/01/26 08:28:50 INFO reduce.MergeManagerImpl: Merging 1 files, 194844 bytes from disk\n",
      "16/01/26 08:28:50 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "16/01/26 08:28:50 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/01/26 08:28:50 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 194836 bytes\n",
      "16/01/26 08:28:50 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/01/26 08:28:50 INFO streaming.PipeMapRed: PipeMapRed exec [/root/hw2/./reducer1.py]\n",
      "16/01/26 08:28:50 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
      "16/01/26 08:28:50 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "16/01/26 08:28:50 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/26 08:28:50 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/26 08:28:50 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/26 08:28:50 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/26 08:28:50 INFO streaming.PipeMapRed: R/W/S=10000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/26 08:28:50 INFO streaming.PipeMapRed: Records R/W=15336/1\n",
      "16/01/26 08:28:50 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/01/26 08:28:50 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/01/26 08:28:50 INFO mapred.Task: Task:attempt_local998646625_0001_r_000000_0 is done. And is in the process of committing\n",
      "16/01/26 08:28:50 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/01/26 08:28:50 INFO mapred.Task: Task attempt_local998646625_0001_r_000000_0 is allowed to commit now\n",
      "16/01/26 08:28:50 INFO output.FileOutputCommitter: Saved output of task 'attempt_local998646625_0001_r_000000_0' to hdfs://localhost:54310/user/root/wk2/hw23/output_1/_temporary/0/task_local998646625_0001_r_000000\n",
      "16/01/26 08:28:50 INFO mapred.LocalJobRunner: Records R/W=15336/1 > reduce\n",
      "16/01/26 08:28:50 INFO mapred.Task: Task 'attempt_local998646625_0001_r_000000_0' done.\n",
      "16/01/26 08:28:50 INFO mapred.LocalJobRunner: Finishing task: attempt_local998646625_0001_r_000000_0\n",
      "16/01/26 08:28:50 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
      "16/01/26 08:28:50 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/01/26 08:28:50 INFO mapreduce.Job: Job job_local998646625_0001 completed successfully\n",
      "16/01/26 08:28:50 INFO mapreduce.Job: Counters: 35\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=601568\n",
      "\t\tFILE: Number of bytes written=1350020\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=407908\n",
      "\t\tHDFS: Number of bytes written=155886\n",
      "\t\tHDFS: Number of read operations=13\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=4\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=100\n",
      "\t\tMap output records=15336\n",
      "\t\tMap output bytes=164166\n",
      "\t\tMap output materialized bytes=194844\n",
      "\t\tInput split bytes=117\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=5741\n",
      "\t\tReduce shuffle bytes=194844\n",
      "\t\tReduce input records=15336\n",
      "\t\tReduce output records=5741\n",
      "\t\tSpilled Records=30672\n",
      "\t\tShuffled Maps =1\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=1\n",
      "\t\tGC time elapsed (ms)=38\n",
      "\t\tTotal committed heap usage (bytes)=1013448704\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=203954\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=155886\n",
      "16/01/26 08:28:50 INFO streaming.StreamJob: Output directory: /user/root/wk2/hw23/output_1\n"
     ]
    }
   ],
   "source": [
    "# Delete output folder if exists\n",
    "!hdfs dfs -rm -r /user/root/wk2/hw23/output_1\n",
    "\n",
    "# Run Hadoop Streaming job\n",
    "!hadoop jar hadoop-streaming-2.7.1.jar \\\n",
    "-mapper mapper1.py \\\n",
    "-reducer reducer1.py \\\n",
    "-input /user/root/wk2/hw23/input \\\n",
    "-output /user/root/wk2/hw23/output_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/26 08:32:11 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/01/26 08:32:12 WARN hdfs.DFSClient: DFSInputStream has been closed already\n"
     ]
    }
   ],
   "source": [
    "!rm part*\n",
    "\n",
    "# Copy the mapper output to local directory\n",
    "!hdfs dfs -copyToLocal /user/root/wk2/hw23/output_1/part*\n",
    "!mv part-00000 hw23out.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/26 08:33:00 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "rm: `/user/root/wk2/hw23/output_2': No such file or directory\n",
      "16/01/26 08:33:03 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/01/26 08:33:04 INFO Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id\n",
      "16/01/26 08:33:04 INFO jvm.JvmMetrics: Initializing JVM Metrics with processName=JobTracker, sessionId=\n",
      "16/01/26 08:33:04 INFO jvm.JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized\n",
      "16/01/26 08:33:04 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/01/26 08:33:04 INFO mapreduce.JobSubmitter: number of splits:1\n",
      "16/01/26 08:33:04 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local1126097630_0001\n",
      "16/01/26 08:33:05 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
      "16/01/26 08:33:05 INFO mapreduce.Job: Running job: job_local1126097630_0001\n",
      "16/01/26 08:33:05 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
      "16/01/26 08:33:05 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
      "16/01/26 08:33:05 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/01/26 08:33:05 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
      "16/01/26 08:33:05 INFO mapred.LocalJobRunner: Starting task: attempt_local1126097630_0001_m_000000_0\n",
      "16/01/26 08:33:05 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/01/26 08:33:05 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
      "16/01/26 08:33:05 INFO mapred.MapTask: Processing split: hdfs://localhost:54310/user/root/wk2/hw23/input/enronemail_1h.txt:0+203954\n",
      "16/01/26 08:33:05 INFO mapred.MapTask: numReduceTasks: 1\n",
      "16/01/26 08:33:05 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "16/01/26 08:33:05 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "16/01/26 08:33:05 INFO mapred.MapTask: soft limit at 83886080\n",
      "16/01/26 08:33:05 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "16/01/26 08:33:05 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "16/01/26 08:33:05 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "16/01/26 08:33:05 INFO streaming.PipeMapRed: PipeMapRed exec [/root/hw2/./mapper2.py]\n",
      "16/01/26 08:33:05 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
      "16/01/26 08:33:05 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
      "16/01/26 08:33:05 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
      "16/01/26 08:33:05 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
      "16/01/26 08:33:05 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
      "16/01/26 08:33:05 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
      "16/01/26 08:33:05 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
      "16/01/26 08:33:05 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
      "16/01/26 08:33:05 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
      "16/01/26 08:33:05 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
      "16/01/26 08:33:05 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
      "16/01/26 08:33:05 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
      "16/01/26 08:33:05 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/26 08:33:05 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/26 08:33:05 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/26 08:33:05 INFO streaming.PipeMapRed: Records R/W=100/1\n",
      "16/01/26 08:33:05 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/01/26 08:33:05 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/01/26 08:33:05 INFO mapred.LocalJobRunner: \n",
      "16/01/26 08:33:05 INFO mapred.MapTask: Starting flush of map output\n",
      "16/01/26 08:33:05 INFO mapred.MapTask: Spilling map output\n",
      "16/01/26 08:33:05 INFO mapred.MapTask: bufstart = 0; bufend = 5275; bufvoid = 104857600\n",
      "16/01/26 08:33:05 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26214000(104856000); length = 397/6553600\n",
      "16/01/26 08:33:05 INFO mapred.MapTask: Finished spill 0\n",
      "16/01/26 08:33:05 INFO mapred.Task: Task:attempt_local1126097630_0001_m_000000_0 is done. And is in the process of committing\n",
      "16/01/26 08:33:05 INFO mapred.LocalJobRunner: Records R/W=100/1\n",
      "16/01/26 08:33:05 INFO mapred.Task: Task 'attempt_local1126097630_0001_m_000000_0' done.\n",
      "16/01/26 08:33:05 INFO mapred.LocalJobRunner: Finishing task: attempt_local1126097630_0001_m_000000_0\n",
      "16/01/26 08:33:05 INFO mapred.LocalJobRunner: map task executor complete.\n",
      "16/01/26 08:33:05 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
      "16/01/26 08:33:05 INFO mapred.LocalJobRunner: Starting task: attempt_local1126097630_0001_r_000000_0\n",
      "16/01/26 08:33:05 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/01/26 08:33:05 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
      "16/01/26 08:33:05 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@714005f6\n",
      "16/01/26 08:33:05 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=353422528, maxSingleShuffleLimit=88355632, mergeThreshold=233258880, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "16/01/26 08:33:05 INFO reduce.EventFetcher: attempt_local1126097630_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "16/01/26 08:33:05 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1126097630_0001_m_000000_0 decomp: 5477 len: 5481 to MEMORY\n",
      "16/01/26 08:33:05 INFO reduce.InMemoryMapOutput: Read 5477 bytes from map-output for attempt_local1126097630_0001_m_000000_0\n",
      "16/01/26 08:33:05 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 5477, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->5477\n",
      "16/01/26 08:33:05 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "16/01/26 08:33:05 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/01/26 08:33:05 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "16/01/26 08:33:05 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/01/26 08:33:05 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 5452 bytes\n",
      "16/01/26 08:33:05 INFO reduce.MergeManagerImpl: Merged 1 segments, 5477 bytes to disk to satisfy reduce memory limit\n",
      "16/01/26 08:33:05 INFO reduce.MergeManagerImpl: Merging 1 files, 5481 bytes from disk\n",
      "16/01/26 08:33:05 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "16/01/26 08:33:05 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/01/26 08:33:05 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 5452 bytes\n",
      "16/01/26 08:33:05 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/01/26 08:33:05 INFO streaming.PipeMapRed: PipeMapRed exec [/root/hw2/./reducer2.py]\n",
      "16/01/26 08:33:05 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
      "16/01/26 08:33:05 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "16/01/26 08:33:06 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/26 08:33:06 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/26 08:33:06 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/26 08:33:06 INFO streaming.PipeMapRed: Records R/W=100/1\n",
      "16/01/26 08:33:06 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/01/26 08:33:06 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/01/26 08:33:06 INFO mapreduce.Job: Job job_local1126097630_0001 running in uber mode : false\n",
      "16/01/26 08:33:06 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/01/26 08:33:06 INFO mapred.Task: Task:attempt_local1126097630_0001_r_000000_0 is done. And is in the process of committing\n",
      "16/01/26 08:33:06 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/01/26 08:33:06 INFO mapred.Task: Task attempt_local1126097630_0001_r_000000_0 is allowed to commit now\n",
      "16/01/26 08:33:06 INFO output.FileOutputCommitter: Saved output of task 'attempt_local1126097630_0001_r_000000_0' to hdfs://localhost:54310/user/root/wk2/hw23/output_2/_temporary/0/task_local1126097630_0001_r_000000\n",
      "16/01/26 08:33:06 INFO mapred.LocalJobRunner: Records R/W=100/1 > reduce\n",
      "16/01/26 08:33:06 INFO mapred.Task: Task 'attempt_local1126097630_0001_r_000000_0' done.\n",
      "16/01/26 08:33:06 INFO mapred.LocalJobRunner: Finishing task: attempt_local1126097630_0001_r_000000_0\n",
      "16/01/26 08:33:06 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
      "16/01/26 08:33:07 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/01/26 08:33:07 INFO mapreduce.Job: Job job_local1126097630_0001 completed successfully\n",
      "16/01/26 08:33:07 INFO mapreduce.Job: Counters: 35\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=222842\n",
      "\t\tFILE: Number of bytes written=784939\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=407908\n",
      "\t\tHDFS: Number of bytes written=5515\n",
      "\t\tHDFS: Number of read operations=13\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=4\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=100\n",
      "\t\tMap output records=100\n",
      "\t\tMap output bytes=5275\n",
      "\t\tMap output materialized bytes=5481\n",
      "\t\tInput split bytes=117\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=100\n",
      "\t\tReduce shuffle bytes=5481\n",
      "\t\tReduce input records=100\n",
      "\t\tReduce output records=202\n",
      "\t\tSpilled Records=200\n",
      "\t\tShuffled Maps =1\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=1\n",
      "\t\tGC time elapsed (ms)=36\n",
      "\t\tTotal committed heap usage (bytes)=1013448704\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=203954\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=5515\n",
      "16/01/26 08:33:07 INFO streaming.StreamJob: Output directory: /user/root/wk2/hw23/output_2\n"
     ]
    }
   ],
   "source": [
    "# Delete output folder if exists\n",
    "!hdfs dfs -rm -r /user/root/wk2/hw23/output_2\n",
    "\n",
    "# Run Hadoop Streaming job\n",
    "!hadoop jar hadoop-streaming-2.7.1.jar \\\n",
    "-mapper mapper2.py \\\n",
    "-reducer reducer2.py \\\n",
    "-input /user/root/wk2/hw23/input \\\n",
    "-output /user/root/wk2/hw23/output_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/26 09:00:21 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "\t\n",
      "Training error: 0.0%\t\n",
      "Accuracy: 100.0%\t\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat /user/root/wk2/hw23/output_2/part-00000|tail -3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/26 08:38:55 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/01/26 08:38:56 WARN hdfs.DFSClient: DFSInputStream has been closed already\n"
     ]
    }
   ],
   "source": [
    "!rm histogram*\n",
    "\n",
    "# Copy the mapper output to local directory\n",
    "!hdfs dfs -copyToLocal /user/root/wk2/hw23/output_2/part*\n",
    "!mv part-00000 histogram.txt\n",
    "\n",
    "!head -n $(($(wc -l < histogram.txt) - 1)) < histogram.txt > histogram_plot.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f2ee268c350>"
      ]
     },
     "execution_count": 281,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f2ee267bc10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeQAAAGHCAYAAABhziENAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f2ee27447d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f2ee268ce50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAekAAAGHCAYAAACUMKq9AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f2ee26ac710>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "df = pd.read_csv(\"histogram_plot.txt\", sep='\\t', header=None)\n",
    "df.columns = ['docid', 'spam', 'predicted', 'pr_spam', 'pr_ham', 'spam0', 'ham0']\n",
    "e_spam = np.exp(df.pr_spam[df.pr_spam!=0])\n",
    "df['e_spam'] = np.where(df['pr_spam']==0, 0.0, np.exp(df['pr_spam']))\n",
    "df['e_ham'] = np.where(df['pr_ham']==0, 0.0, np.exp(df['pr_ham']))\n",
    "\n",
    "plt.figure()\n",
    "plt.figure(figsize=(8,6), dpi=30)\n",
    "plt.title('Probability distribution for Spam')\n",
    "df.e_spam.hist(alpha=0.5)\n",
    "\n",
    "plt.figure()\n",
    "plt.figure(figsize=(8,6), dpi=30)\n",
    "plt.title('Probability distribution for Ham')\n",
    "df.e_ham.hist(alpha=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW2.4. \n",
    "Repeat HW2.3 with the following modification: use Laplace plus-one smoothing. Compare the misclassifcation error rates for 2.3 versus 2.4 and explain the differences.\n",
    "\n",
    "For a quick reference on the construction of the Multinomial NAIVE BAYES classifier that you will code,\n",
    "please consult the \"Document Classification\" section of the following wikipedia page:\n",
    "\n",
    "https://en.wikipedia.org/wiki/Naive_Bayes_classifier#Document_classification\n",
    "\n",
    "OR the original paper by the curators of the Enron email data:\n",
    "\n",
    "http://www.aueb.gr/users/ion/docs/ceas2006_paper.pdf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The problem statement is similar to HW2.3 and therefore the structure for this problem is almost the same except for reducer1.py. We will reuse the remaining mappers and reducers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing reducer3.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer3.py\n",
    "#!/usr/bin/python\n",
    "## reducer3.py\n",
    "## Author: Prabhakar Gundugola\n",
    "## Description: reducer3 code for HW2.4\n",
    "\n",
    "import sys\n",
    "import math\n",
    "\n",
    "spam_email_count = 0\n",
    "ham_email_count = 0\n",
    "\n",
    "spam_word_count = 0\n",
    "ham_word_count = 0\n",
    "\n",
    "total_spam_count = 0\n",
    "total_ham_count = 0\n",
    "\n",
    "total_docs = 0\n",
    "\n",
    "spam_words_freq = {}\n",
    "ham_words_freq = {}\n",
    "\n",
    "priors_calc = 0\n",
    "\n",
    "for line in sys.stdin:\n",
    "    tokens = line.strip().split('\\t')\n",
    "    \n",
    "    if tokens[0] == \"0000000DOC_CLASS\":\n",
    "        spam_email_count = int(tokens[1])\n",
    "        ham_email_count = int(tokens[2])\n",
    "        continue\n",
    "    \n",
    "    try:\n",
    "        count = int(tokens[2])\n",
    "        spam = int(tokens[1])\n",
    "    except ValueError:\n",
    "        continue\n",
    "   \n",
    "    \n",
    "    word, spam, frequency = tokens[0], spam, count\n",
    "    if spam == 1:\n",
    "        total_spam_count += frequency\n",
    "        if word in spam_words_freq:\n",
    "            spam_words_freq[word] += frequency\n",
    "        else:\n",
    "            spam_words_freq[word] = frequency\n",
    "        \n",
    "        if word not in ham_words_freq:\n",
    "            ham_words_freq[word] = 0\n",
    "    else:\n",
    "        total_ham_count += frequency\n",
    "        if word in ham_words_freq:\n",
    "            ham_words_freq[word] += frequency\n",
    "        else:\n",
    "            ham_words_freq[word] = frequency\n",
    "        \n",
    "        if word not in spam_words_freq:\n",
    "            spam_words_freq[word] = 0\n",
    "            \n",
    "prior_spam = math.log(1.0*spam_email_count/(spam_email_count + ham_email_count))\n",
    "prior_ham = math.log(1.0*ham_email_count/(spam_email_count + ham_email_count))   \n",
    "print('{0}\\t{1}\\t{2}'.format(\"0000000PRIORS\", str(prior_spam), str(prior_ham)))\n",
    "\n",
    "\n",
    "# Calculate conditional probability. \n",
    "# Applied Laplace smoothing to math.log function in the numerator\n",
    "prob_spam_words = {}\n",
    "prob_ham_words = {}\n",
    "\n",
    "for word in ham_words_freq:\n",
    "    if spam_words_freq[word] > 0:\n",
    "        prob_spam_words[word] = math.log((1.0)*(spam_words_freq[word]+1)/total_spam_count)\n",
    "    else:\n",
    "        prob_spam_words[word] = 0\n",
    "    if ham_words_freq[word] > 0:\n",
    "        prob_ham_words[word] = math.log((1.0)*(ham_words_freq[word]+1)/total_ham_count)\n",
    "    else:\n",
    "        prob_ham_words[word] = 0\n",
    "    \n",
    "    print('{0}\\t{1}\\t{2}'.format(word, str(prob_spam_words[word]), str(prob_ham_words[word])))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/26 08:56:09 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "rm: `/user/root/wk2/hw24': No such file or directory\n",
      "16/01/26 08:56:12 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/01/26 08:56:15 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# Ensure the input folder doesn't exist\n",
    "!hdfs dfs -rm -r /user/root/wk2/hw24\n",
    "!hdfs dfs -mkdir -p /user/root/wk2/hw24/input\n",
    "\n",
    "!hdfs dfs -put enronemail_1h.txt /user/root/wk2/hw24/input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/26 08:56:51 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "rm: `/user/root/wk2/hw24/output_1': No such file or directory\n",
      "16/01/26 08:56:54 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/01/26 08:56:55 INFO Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id\n",
      "16/01/26 08:56:55 INFO jvm.JvmMetrics: Initializing JVM Metrics with processName=JobTracker, sessionId=\n",
      "16/01/26 08:56:55 INFO jvm.JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized\n",
      "16/01/26 08:56:55 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/01/26 08:56:55 INFO mapreduce.JobSubmitter: number of splits:1\n",
      "16/01/26 08:56:55 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local392316276_0001\n",
      "16/01/26 08:56:56 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
      "16/01/26 08:56:56 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
      "16/01/26 08:56:56 INFO mapreduce.Job: Running job: job_local392316276_0001\n",
      "16/01/26 08:56:56 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
      "16/01/26 08:56:56 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/01/26 08:56:56 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
      "16/01/26 08:56:56 INFO mapred.LocalJobRunner: Starting task: attempt_local392316276_0001_m_000000_0\n",
      "16/01/26 08:56:56 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/01/26 08:56:56 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
      "16/01/26 08:56:56 INFO mapred.MapTask: Processing split: hdfs://localhost:54310/user/root/wk2/hw24/input/enronemail_1h.txt:0+203954\n",
      "16/01/26 08:56:56 INFO mapred.MapTask: numReduceTasks: 1\n",
      "16/01/26 08:56:56 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "16/01/26 08:56:56 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "16/01/26 08:56:56 INFO mapred.MapTask: soft limit at 83886080\n",
      "16/01/26 08:56:56 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "16/01/26 08:56:56 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "16/01/26 08:56:56 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "16/01/26 08:56:56 INFO streaming.PipeMapRed: PipeMapRed exec [/root/hw2/./mapper1.py]\n",
      "16/01/26 08:56:56 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
      "16/01/26 08:56:56 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
      "16/01/26 08:56:56 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
      "16/01/26 08:56:56 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
      "16/01/26 08:56:56 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
      "16/01/26 08:56:56 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
      "16/01/26 08:56:56 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
      "16/01/26 08:56:56 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
      "16/01/26 08:56:56 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
      "16/01/26 08:56:56 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
      "16/01/26 08:56:56 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
      "16/01/26 08:56:56 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
      "16/01/26 08:56:56 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/26 08:56:56 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/26 08:56:56 INFO streaming.PipeMapRed: Records R/W=72/1\n",
      "16/01/26 08:56:56 INFO streaming.PipeMapRed: R/W/S=100/2164/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/26 08:56:56 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/01/26 08:56:56 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/01/26 08:56:56 INFO mapred.LocalJobRunner: \n",
      "16/01/26 08:56:56 INFO mapred.MapTask: Starting flush of map output\n",
      "16/01/26 08:56:56 INFO mapred.MapTask: Spilling map output\n",
      "16/01/26 08:56:56 INFO mapred.MapTask: bufstart = 0; bufend = 164166; bufvoid = 104857600\n",
      "16/01/26 08:56:56 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26153056(104612224); length = 61341/6553600\n",
      "16/01/26 08:56:57 INFO mapreduce.Job: Job job_local392316276_0001 running in uber mode : false\n",
      "16/01/26 08:56:57 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/01/26 08:56:57 INFO mapred.MapTask: Finished spill 0\n",
      "16/01/26 08:56:57 INFO mapred.Task: Task:attempt_local392316276_0001_m_000000_0 is done. And is in the process of committing\n",
      "16/01/26 08:56:57 INFO mapred.LocalJobRunner: Records R/W=72/1\n",
      "16/01/26 08:56:57 INFO mapred.Task: Task 'attempt_local392316276_0001_m_000000_0' done.\n",
      "16/01/26 08:56:57 INFO mapred.LocalJobRunner: Finishing task: attempt_local392316276_0001_m_000000_0\n",
      "16/01/26 08:56:57 INFO mapred.LocalJobRunner: map task executor complete.\n",
      "16/01/26 08:56:57 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
      "16/01/26 08:56:57 INFO mapred.LocalJobRunner: Starting task: attempt_local392316276_0001_r_000000_0\n",
      "16/01/26 08:56:57 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/01/26 08:56:57 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
      "16/01/26 08:56:57 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@1ece5b86\n",
      "16/01/26 08:56:57 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=353422528, maxSingleShuffleLimit=88355632, mergeThreshold=233258880, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "16/01/26 08:56:57 INFO reduce.EventFetcher: attempt_local392316276_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "16/01/26 08:56:57 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local392316276_0001_m_000000_0 decomp: 194840 len: 194844 to MEMORY\n",
      "16/01/26 08:56:57 INFO reduce.InMemoryMapOutput: Read 194840 bytes from map-output for attempt_local392316276_0001_m_000000_0\n",
      "16/01/26 08:56:57 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 194840, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->194840\n",
      "16/01/26 08:56:57 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "16/01/26 08:56:57 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/01/26 08:56:57 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "16/01/26 08:56:57 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/01/26 08:56:57 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 194836 bytes\n",
      "16/01/26 08:56:57 INFO reduce.MergeManagerImpl: Merged 1 segments, 194840 bytes to disk to satisfy reduce memory limit\n",
      "16/01/26 08:56:57 INFO reduce.MergeManagerImpl: Merging 1 files, 194844 bytes from disk\n",
      "16/01/26 08:56:57 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "16/01/26 08:56:57 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/01/26 08:56:57 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 194836 bytes\n",
      "16/01/26 08:56:57 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/01/26 08:56:57 INFO streaming.PipeMapRed: PipeMapRed exec [/root/hw2/./reducer3.py]\n",
      "16/01/26 08:56:57 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
      "16/01/26 08:56:57 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "16/01/26 08:56:57 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/26 08:56:57 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/26 08:56:57 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/26 08:56:57 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/26 08:56:57 INFO streaming.PipeMapRed: R/W/S=10000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/26 08:56:57 INFO streaming.PipeMapRed: Records R/W=15336/1\n",
      "16/01/26 08:56:57 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/01/26 08:56:57 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/01/26 08:56:58 INFO mapred.Task: Task:attempt_local392316276_0001_r_000000_0 is done. And is in the process of committing\n",
      "16/01/26 08:56:58 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/01/26 08:56:58 INFO mapred.Task: Task attempt_local392316276_0001_r_000000_0 is allowed to commit now\n",
      "16/01/26 08:56:58 INFO output.FileOutputCommitter: Saved output of task 'attempt_local392316276_0001_r_000000_0' to hdfs://localhost:54310/user/root/wk2/hw24/output_1/_temporary/0/task_local392316276_0001_r_000000\n",
      "16/01/26 08:56:58 INFO mapred.LocalJobRunner: Records R/W=15336/1 > reduce\n",
      "16/01/26 08:56:58 INFO mapred.Task: Task 'attempt_local392316276_0001_r_000000_0' done.\n",
      "16/01/26 08:56:58 INFO mapred.LocalJobRunner: Finishing task: attempt_local392316276_0001_r_000000_0\n",
      "16/01/26 08:56:58 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
      "16/01/26 08:56:58 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/01/26 08:56:58 INFO mapreduce.Job: Job job_local392316276_0001 completed successfully\n",
      "16/01/26 08:56:58 INFO mapreduce.Job: Counters: 35\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=601568\n",
      "\t\tFILE: Number of bytes written=1350020\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=407908\n",
      "\t\tHDFS: Number of bytes written=155887\n",
      "\t\tHDFS: Number of read operations=13\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=4\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=100\n",
      "\t\tMap output records=15336\n",
      "\t\tMap output bytes=164166\n",
      "\t\tMap output materialized bytes=194844\n",
      "\t\tInput split bytes=117\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=5741\n",
      "\t\tReduce shuffle bytes=194844\n",
      "\t\tReduce input records=15336\n",
      "\t\tReduce output records=5741\n",
      "\t\tSpilled Records=30672\n",
      "\t\tShuffled Maps =1\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=1\n",
      "\t\tGC time elapsed (ms)=49\n",
      "\t\tTotal committed heap usage (bytes)=1013448704\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=203954\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=155887\n",
      "16/01/26 08:56:58 INFO streaming.StreamJob: Output directory: /user/root/wk2/hw24/output_1\n"
     ]
    }
   ],
   "source": [
    "# Delete output folder if exists\n",
    "!hdfs dfs -rm -r /user/root/wk2/hw24/output_1\n",
    "\n",
    "# Run Hadoop Streaming job\n",
    "!hadoop jar hadoop-streaming-2.7.1.jar \\\n",
    "-mapper mapper1.py \\\n",
    "-reducer reducer3.py \\\n",
    "-input /user/root/wk2/hw24/input \\\n",
    "-output /user/root/wk2/hw24/output_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: cannot remove 'part*': No such file or directory\n",
      "16/01/26 08:57:36 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/01/26 08:57:37 WARN hdfs.DFSClient: DFSInputStream has been closed already\n"
     ]
    }
   ],
   "source": [
    "!rm part*\n",
    "\n",
    "# Copy the mapper output to local directory\n",
    "!hdfs dfs -copyToLocal /user/root/wk2/hw24/output_1/part*\n",
    "!mv part-00000 hw23out.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/26 08:58:23 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "rm: `/user/root/wk2/hw24/output_2': No such file or directory\n",
      "16/01/26 08:58:26 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/01/26 08:58:27 INFO Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id\n",
      "16/01/26 08:58:27 INFO jvm.JvmMetrics: Initializing JVM Metrics with processName=JobTracker, sessionId=\n",
      "16/01/26 08:58:27 INFO jvm.JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized\n",
      "16/01/26 08:58:28 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/01/26 08:58:28 INFO mapreduce.JobSubmitter: number of splits:1\n",
      "16/01/26 08:58:28 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local214088061_0001\n",
      "16/01/26 08:58:28 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
      "16/01/26 08:58:28 INFO mapreduce.Job: Running job: job_local214088061_0001\n",
      "16/01/26 08:58:28 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
      "16/01/26 08:58:28 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
      "16/01/26 08:58:28 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/01/26 08:58:28 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
      "16/01/26 08:58:28 INFO mapred.LocalJobRunner: Starting task: attempt_local214088061_0001_m_000000_0\n",
      "16/01/26 08:58:28 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/01/26 08:58:28 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
      "16/01/26 08:58:28 INFO mapred.MapTask: Processing split: hdfs://localhost:54310/user/root/wk2/hw24/input/enronemail_1h.txt:0+203954\n",
      "16/01/26 08:58:28 INFO mapred.MapTask: numReduceTasks: 1\n",
      "16/01/26 08:58:28 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "16/01/26 08:58:28 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "16/01/26 08:58:28 INFO mapred.MapTask: soft limit at 83886080\n",
      "16/01/26 08:58:28 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "16/01/26 08:58:28 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "16/01/26 08:58:28 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "16/01/26 08:58:28 INFO streaming.PipeMapRed: PipeMapRed exec [/root/hw2/./mapper2.py]\n",
      "16/01/26 08:58:28 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
      "16/01/26 08:58:28 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
      "16/01/26 08:58:28 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
      "16/01/26 08:58:28 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
      "16/01/26 08:58:28 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
      "16/01/26 08:58:29 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
      "16/01/26 08:58:29 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
      "16/01/26 08:58:29 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
      "16/01/26 08:58:29 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
      "16/01/26 08:58:29 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
      "16/01/26 08:58:29 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
      "16/01/26 08:58:29 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
      "16/01/26 08:58:29 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/26 08:58:29 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/26 08:58:29 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/26 08:58:29 INFO streaming.PipeMapRed: Records R/W=100/1\n",
      "16/01/26 08:58:29 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/01/26 08:58:29 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/01/26 08:58:29 INFO mapred.LocalJobRunner: \n",
      "16/01/26 08:58:29 INFO mapred.MapTask: Starting flush of map output\n",
      "16/01/26 08:58:29 INFO mapred.MapTask: Spilling map output\n",
      "16/01/26 08:58:29 INFO mapred.MapTask: bufstart = 0; bufend = 5277; bufvoid = 104857600\n",
      "16/01/26 08:58:29 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26214000(104856000); length = 397/6553600\n",
      "16/01/26 08:58:29 INFO mapred.MapTask: Finished spill 0\n",
      "16/01/26 08:58:29 INFO mapred.Task: Task:attempt_local214088061_0001_m_000000_0 is done. And is in the process of committing\n",
      "16/01/26 08:58:29 INFO mapred.LocalJobRunner: Records R/W=100/1\n",
      "16/01/26 08:58:29 INFO mapred.Task: Task 'attempt_local214088061_0001_m_000000_0' done.\n",
      "16/01/26 08:58:29 INFO mapred.LocalJobRunner: Finishing task: attempt_local214088061_0001_m_000000_0\n",
      "16/01/26 08:58:29 INFO mapred.LocalJobRunner: map task executor complete.\n",
      "16/01/26 08:58:29 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
      "16/01/26 08:58:29 INFO mapred.LocalJobRunner: Starting task: attempt_local214088061_0001_r_000000_0\n",
      "16/01/26 08:58:29 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/01/26 08:58:29 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
      "16/01/26 08:58:29 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@2956842e\n",
      "16/01/26 08:58:29 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=353422528, maxSingleShuffleLimit=88355632, mergeThreshold=233258880, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "16/01/26 08:58:29 INFO reduce.EventFetcher: attempt_local214088061_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "16/01/26 08:58:29 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local214088061_0001_m_000000_0 decomp: 5479 len: 5483 to MEMORY\n",
      "16/01/26 08:58:29 INFO reduce.InMemoryMapOutput: Read 5479 bytes from map-output for attempt_local214088061_0001_m_000000_0\n",
      "16/01/26 08:58:29 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 5479, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->5479\n",
      "16/01/26 08:58:29 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "16/01/26 08:58:29 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/01/26 08:58:29 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "16/01/26 08:58:29 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/01/26 08:58:29 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 5454 bytes\n",
      "16/01/26 08:58:29 INFO reduce.MergeManagerImpl: Merged 1 segments, 5479 bytes to disk to satisfy reduce memory limit\n",
      "16/01/26 08:58:29 INFO reduce.MergeManagerImpl: Merging 1 files, 5483 bytes from disk\n",
      "16/01/26 08:58:29 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "16/01/26 08:58:29 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/01/26 08:58:29 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 5454 bytes\n",
      "16/01/26 08:58:29 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/01/26 08:58:29 INFO streaming.PipeMapRed: PipeMapRed exec [/root/hw2/./reducer2.py]\n",
      "16/01/26 08:58:29 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
      "16/01/26 08:58:29 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "16/01/26 08:58:29 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/26 08:58:29 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/26 08:58:29 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/26 08:58:29 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/01/26 08:58:29 INFO streaming.PipeMapRed: Records R/W=100/1\n",
      "16/01/26 08:58:29 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/01/26 08:58:29 INFO mapred.Task: Task:attempt_local214088061_0001_r_000000_0 is done. And is in the process of committing\n",
      "16/01/26 08:58:29 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/01/26 08:58:29 INFO mapred.Task: Task attempt_local214088061_0001_r_000000_0 is allowed to commit now\n",
      "16/01/26 08:58:29 INFO mapreduce.Job: Job job_local214088061_0001 running in uber mode : false\n",
      "16/01/26 08:58:29 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/01/26 08:58:29 INFO output.FileOutputCommitter: Saved output of task 'attempt_local214088061_0001_r_000000_0' to hdfs://localhost:54310/user/root/wk2/hw24/output_2/_temporary/0/task_local214088061_0001_r_000000\n",
      "16/01/26 08:58:29 INFO mapred.LocalJobRunner: Records R/W=100/1 > reduce\n",
      "16/01/26 08:58:29 INFO mapred.Task: Task 'attempt_local214088061_0001_r_000000_0' done.\n",
      "16/01/26 08:58:29 INFO mapred.LocalJobRunner: Finishing task: attempt_local214088061_0001_r_000000_0\n",
      "16/01/26 08:58:29 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
      "16/01/26 08:58:30 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/01/26 08:58:30 INFO mapreduce.Job: Job job_local214088061_0001 completed successfully\n",
      "16/01/26 08:58:30 INFO mapreduce.Job: Counters: 35\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=222846\n",
      "\t\tFILE: Number of bytes written=781937\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=407908\n",
      "\t\tHDFS: Number of bytes written=5517\n",
      "\t\tHDFS: Number of read operations=13\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=4\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=100\n",
      "\t\tMap output records=100\n",
      "\t\tMap output bytes=5277\n",
      "\t\tMap output materialized bytes=5483\n",
      "\t\tInput split bytes=117\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=100\n",
      "\t\tReduce shuffle bytes=5483\n",
      "\t\tReduce input records=100\n",
      "\t\tReduce output records=202\n",
      "\t\tSpilled Records=200\n",
      "\t\tShuffled Maps =1\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=1\n",
      "\t\tGC time elapsed (ms)=34\n",
      "\t\tTotal committed heap usage (bytes)=1013448704\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=203954\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=5517\n",
      "16/01/26 08:58:30 INFO streaming.StreamJob: Output directory: /user/root/wk2/hw24/output_2\n"
     ]
    }
   ],
   "source": [
    "# Delete output folder if exists\n",
    "!hdfs dfs -rm -r /user/root/wk2/hw24/output_2\n",
    "\n",
    "# Run Hadoop Streaming job\n",
    "!hadoop jar hadoop-streaming-2.7.1.jar \\\n",
    "-mapper mapper2.py \\\n",
    "-reducer reducer2.py \\\n",
    "-input /user/root/wk2/hw24/input \\\n",
    "-output /user/root/wk2/hw24/output_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/26 09:00:46 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "\t\n",
      "Training error: 0.0%\t\n",
      "Accuracy: 100.0%\t\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat /user/root/wk2/hw24/output_2/part-00000|tail -3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW 2.5.\n",
    "Repeat HW2.4. This time when modeling and classification ignore tokens with a frequency of less than three (3) in the training set. How does it affect the misclassifcation error of learnt naive multinomial Bayesian Classifier on the training dataset:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We will reuse HW2.3 mappers and reducers except reducer1.py. **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer4.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer4.py\n",
    "#!/usr/bin/python\n",
    "## reducer4.py\n",
    "## Author: Prabhakar Gundugola\n",
    "## Description: reducer4 code for HW2.5\n",
    "\n",
    "import sys\n",
    "import math\n",
    "\n",
    "spam_email_count = 0\n",
    "ham_email_count = 0\n",
    "\n",
    "spam_word_count = 0\n",
    "ham_word_count = 0\n",
    "\n",
    "total_spam_count = 0\n",
    "total_ham_count = 0\n",
    "total_word_count = 0\n",
    "\n",
    "total_docs = 0\n",
    "\n",
    "spam_words_freq = {}\n",
    "ham_words_freq = {}\n",
    "\n",
    "priors_calc = 0\n",
    "\n",
    "for line in sys.stdin:\n",
    "    tokens = line.strip().split('\\t')\n",
    "    \n",
    "    if tokens[0] == \"0000000DOC_CLASS\":\n",
    "        spam_email_count = int(tokens[1])\n",
    "        ham_email_count = int(tokens[2])\n",
    "        continue\n",
    "    \n",
    "    try:\n",
    "        count = int(tokens[2])\n",
    "        spam = int(tokens[1])\n",
    "    except ValueError:\n",
    "        continue\n",
    "   \n",
    "    \n",
    "    word, spam, frequency = tokens[0], spam, count\n",
    "    \n",
    "    if not(word in spam_words_freq or word in ham_words_freq):\n",
    "        total_word_count += 1\n",
    "    \n",
    "    \n",
    "    if spam == 1:\n",
    "        total_spam_count += frequency\n",
    "        if word in spam_words_freq:\n",
    "            spam_words_freq[word] += frequency\n",
    "        else:\n",
    "            spam_words_freq[word] = frequency\n",
    "        \n",
    "        if word not in ham_words_freq:\n",
    "            ham_words_freq[word] = 0\n",
    "    else:\n",
    "        total_ham_count += frequency\n",
    "        if word in ham_words_freq:\n",
    "            ham_words_freq[word] += frequency\n",
    "        else:\n",
    "            ham_words_freq[word] = frequency\n",
    "        \n",
    "        if word not in spam_words_freq:\n",
    "            spam_words_freq[word] = 0\n",
    "            \n",
    "prior_spam = math.log(1.0*spam_email_count/(spam_email_count + ham_email_count))\n",
    "prior_ham = math.log(1.0*ham_email_count/(spam_email_count + ham_email_count))   \n",
    "print('{0}\\t{1}\\t{2}'.format(\"0000000PRIORS\", str(prior_spam), str(prior_ham)))\n",
    "\n",
    "\n",
    "# Calculate conditional probability. \n",
    "# Applied Laplace smoothing to math.log function in the numerator\n",
    "prob_spam_words = {}\n",
    "prob_ham_words = {}\n",
    "\n",
    "for word in ham_words_freq:\n",
    "    ham_count = ham_words_freq[word]\n",
    "    spam_count = spam_words_freq[word]\n",
    "    if (ham_count+spam_count < 3 ):\n",
    "        total_word_count -= 1\n",
    "        continue\n",
    "    if spam_words_freq[word] > 0:\n",
    "        prob_spam_words[word] = math.log((1.0)*(spam_words_freq[word]+1)/total_spam_count + total_word_count)\n",
    "    else:\n",
    "        prob_spam_words[word] = 0\n",
    "    if ham_words_freq[word] > 0:\n",
    "        prob_ham_words[word] = math.log((1.0)*(ham_words_freq[word]+1)/total_ham_count + total_word_count)\n",
    "    else:\n",
    "        prob_ham_words[word] = 0\n",
    "    \n",
    "    print('{0}\\t{1}\\t{2}'.format(word, str(prob_spam_words[word]), str(prob_ham_words[word])))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/26 09:42:30 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/01/26 09:42:31 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /user/root/wk2/hw25\n",
      "16/01/26 09:42:33 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/01/26 09:42:36 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# Ensure the input folder doesn't exist\n",
    "!hdfs dfs -rm -r /user/root/wk2/hw25\n",
    "!hdfs dfs -mkdir -p /user/root/wk2/hw25/input\n",
    "\n",
    "!hdfs dfs -put enronemail_1h.txt /user/root/wk2/hw25/input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/26 09:42:39 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "rm: `/user/root/wk2/hw25/output_1': No such file or directory\n",
      "16/01/26 09:42:42 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/01/26 09:42:43 INFO Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id\n",
      "16/01/26 09:42:43 INFO jvm.JvmMetrics: Initializing JVM Metrics with processName=JobTracker, sessionId=\n",
      "16/01/26 09:42:43 INFO jvm.JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized\n",
      "16/01/26 09:42:43 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/01/26 09:42:43 INFO mapreduce.JobSubmitter: number of splits:1\n",
      "16/01/26 09:42:43 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local1511549212_0001\n",
      "16/01/26 09:42:43 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
      "16/01/26 09:42:43 INFO mapreduce.Job: Running job: job_local1511549212_0001\n",
      "16/01/26 09:42:43 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
      "16/01/26 09:42:43 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
      "16/01/26 09:42:43 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/01/26 09:42:44 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
      "16/01/26 09:42:44 INFO mapred.LocalJobRunner: Starting task: attempt_local1511549212_0001_m_000000_0\n",
      "16/01/26 09:42:44 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/01/26 09:42:44 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
      "16/01/26 09:42:44 INFO mapred.MapTask: Processing split: hdfs://localhost:54310/user/root/wk2/hw25/input/enronemail_1h.txt:0+203954\n",
      "16/01/26 09:42:44 INFO mapred.MapTask: numReduceTasks: 1\n",
      "16/01/26 09:42:44 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "16/01/26 09:42:44 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "16/01/26 09:42:44 INFO mapred.MapTask: soft limit at 83886080\n",
      "16/01/26 09:42:44 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "16/01/26 09:42:44 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "16/01/26 09:42:44 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "16/01/26 09:42:44 INFO streaming.PipeMapRed: PipeMapRed exec [/root/hw2/./mapper1.py]\n",
      "16/01/26 09:42:44 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
      "16/01/26 09:42:44 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
      "16/01/26 09:42:44 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
      "16/01/26 09:42:44 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
      "16/01/26 09:42:44 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
      "16/01/26 09:42:44 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
      "16/01/26 09:42:44 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
      "16/01/26 09:42:44 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
      "16/01/26 09:42:44 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
      "16/01/26 09:42:44 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
      "16/01/26 09:42:44 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
      "16/01/26 09:42:44 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
      "16/01/26 09:42:44 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/26 09:42:44 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/26 09:42:44 INFO streaming.PipeMapRed: Records R/W=72/1\n",
      "16/01/26 09:42:44 INFO streaming.PipeMapRed: R/W/S=100/2196/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/26 09:42:44 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/01/26 09:42:44 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/01/26 09:42:44 INFO mapred.LocalJobRunner: \n",
      "16/01/26 09:42:44 INFO mapred.MapTask: Starting flush of map output\n",
      "16/01/26 09:42:44 INFO mapred.MapTask: Spilling map output\n",
      "16/01/26 09:42:44 INFO mapred.MapTask: bufstart = 0; bufend = 164166; bufvoid = 104857600\n",
      "16/01/26 09:42:44 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26153056(104612224); length = 61341/6553600\n",
      "16/01/26 09:42:44 INFO mapred.MapTask: Finished spill 0\n",
      "16/01/26 09:42:44 INFO mapreduce.Job: Job job_local1511549212_0001 running in uber mode : false\n",
      "16/01/26 09:42:44 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/01/26 09:42:44 INFO mapred.Task: Task:attempt_local1511549212_0001_m_000000_0 is done. And is in the process of committing\n",
      "16/01/26 09:42:44 INFO mapred.LocalJobRunner: Records R/W=72/1\n",
      "16/01/26 09:42:44 INFO mapred.Task: Task 'attempt_local1511549212_0001_m_000000_0' done.\n",
      "16/01/26 09:42:44 INFO mapred.LocalJobRunner: Finishing task: attempt_local1511549212_0001_m_000000_0\n",
      "16/01/26 09:42:44 INFO mapred.LocalJobRunner: map task executor complete.\n",
      "16/01/26 09:42:44 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
      "16/01/26 09:42:44 INFO mapred.LocalJobRunner: Starting task: attempt_local1511549212_0001_r_000000_0\n",
      "16/01/26 09:42:44 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/01/26 09:42:44 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
      "16/01/26 09:42:44 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@2711dfc7\n",
      "16/01/26 09:42:45 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=353422528, maxSingleShuffleLimit=88355632, mergeThreshold=233258880, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "16/01/26 09:42:45 INFO reduce.EventFetcher: attempt_local1511549212_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "16/01/26 09:42:45 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1511549212_0001_m_000000_0 decomp: 194840 len: 194844 to MEMORY\n",
      "16/01/26 09:42:45 INFO reduce.InMemoryMapOutput: Read 194840 bytes from map-output for attempt_local1511549212_0001_m_000000_0\n",
      "16/01/26 09:42:45 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 194840, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->194840\n",
      "16/01/26 09:42:45 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "16/01/26 09:42:45 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/01/26 09:42:45 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "16/01/26 09:42:45 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/01/26 09:42:45 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 194836 bytes\n",
      "16/01/26 09:42:45 INFO reduce.MergeManagerImpl: Merged 1 segments, 194840 bytes to disk to satisfy reduce memory limit\n",
      "16/01/26 09:42:45 INFO reduce.MergeManagerImpl: Merging 1 files, 194844 bytes from disk\n",
      "16/01/26 09:42:45 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "16/01/26 09:42:45 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/01/26 09:42:45 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 194836 bytes\n",
      "16/01/26 09:42:45 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/01/26 09:42:45 INFO streaming.PipeMapRed: PipeMapRed exec [/root/hw2/./reducer4.py]\n",
      "16/01/26 09:42:45 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
      "16/01/26 09:42:45 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "16/01/26 09:42:45 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/26 09:42:45 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/26 09:42:45 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/26 09:42:45 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/26 09:42:45 INFO streaming.PipeMapRed: R/W/S=10000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/26 09:42:45 INFO streaming.PipeMapRed: Records R/W=15336/1\n",
      "16/01/26 09:42:45 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/01/26 09:42:45 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/01/26 09:42:45 INFO mapred.Task: Task:attempt_local1511549212_0001_r_000000_0 is done. And is in the process of committing\n",
      "16/01/26 09:42:45 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/01/26 09:42:45 INFO mapred.Task: Task attempt_local1511549212_0001_r_000000_0 is allowed to commit now\n",
      "16/01/26 09:42:45 INFO output.FileOutputCommitter: Saved output of task 'attempt_local1511549212_0001_r_000000_0' to hdfs://localhost:54310/user/root/wk2/hw25/output_1/_temporary/0/task_local1511549212_0001_r_000000\n",
      "16/01/26 09:42:45 INFO mapred.LocalJobRunner: Records R/W=15336/1 > reduce\n",
      "16/01/26 09:42:45 INFO mapred.Task: Task 'attempt_local1511549212_0001_r_000000_0' done.\n",
      "16/01/26 09:42:45 INFO mapred.LocalJobRunner: Finishing task: attempt_local1511549212_0001_r_000000_0\n",
      "16/01/26 09:42:45 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
      "16/01/26 09:42:45 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/01/26 09:42:45 INFO mapreduce.Job: Job job_local1511549212_0001 completed successfully\n",
      "16/01/26 09:42:45 INFO mapreduce.Job: Counters: 35\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=601568\n",
      "\t\tFILE: Number of bytes written=1353028\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=407908\n",
      "\t\tHDFS: Number of bytes written=53028\n",
      "\t\tHDFS: Number of read operations=13\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=4\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=100\n",
      "\t\tMap output records=15336\n",
      "\t\tMap output bytes=164166\n",
      "\t\tMap output materialized bytes=194844\n",
      "\t\tInput split bytes=117\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=5741\n",
      "\t\tReduce shuffle bytes=194844\n",
      "\t\tReduce input records=15336\n",
      "\t\tReduce output records=1830\n",
      "\t\tSpilled Records=30672\n",
      "\t\tShuffled Maps =1\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=1\n",
      "\t\tGC time elapsed (ms)=33\n",
      "\t\tTotal committed heap usage (bytes)=1013448704\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=203954\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=53028\n",
      "16/01/26 09:42:45 INFO streaming.StreamJob: Output directory: /user/root/wk2/hw25/output_1\n"
     ]
    }
   ],
   "source": [
    "# Delete output folder if exists\n",
    "!hdfs dfs -rm -r /user/root/wk2/hw25/output_1\n",
    "\n",
    "# Run Hadoop Streaming job\n",
    "!hadoop jar hadoop-streaming-2.7.1.jar \\\n",
    "-mapper mapper1.py \\\n",
    "-reducer reducer4.py \\\n",
    "-input /user/root/wk2/hw25/input \\\n",
    "-output /user/root/wk2/hw25/output_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: cannot remove 'hw23out.txt': No such file or directory\n",
      "16/01/26 09:43:10 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/01/26 09:43:11 WARN hdfs.DFSClient: DFSInputStream has been closed already\n"
     ]
    }
   ],
   "source": [
    "!rm hw23out.txt\n",
    "\n",
    "# Copy the mapper output to local directory\n",
    "!hdfs dfs -copyToLocal /user/root/wk2/hw25/output_1/part*\n",
    "!mv part-00000 hw23out.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/26 09:43:17 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "rm: `/user/root/wk2/hw25/output_2': No such file or directory\n",
      "16/01/26 09:43:20 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/01/26 09:43:21 INFO Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id\n",
      "16/01/26 09:43:21 INFO jvm.JvmMetrics: Initializing JVM Metrics with processName=JobTracker, sessionId=\n",
      "16/01/26 09:43:21 INFO jvm.JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized\n",
      "16/01/26 09:43:22 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/01/26 09:43:22 INFO mapreduce.JobSubmitter: number of splits:1\n",
      "16/01/26 09:43:22 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local1707625718_0001\n",
      "16/01/26 09:43:22 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
      "16/01/26 09:43:22 INFO mapreduce.Job: Running job: job_local1707625718_0001\n",
      "16/01/26 09:43:22 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
      "16/01/26 09:43:22 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
      "16/01/26 09:43:22 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/01/26 09:43:22 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
      "16/01/26 09:43:22 INFO mapred.LocalJobRunner: Starting task: attempt_local1707625718_0001_m_000000_0\n",
      "16/01/26 09:43:22 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/01/26 09:43:22 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
      "16/01/26 09:43:22 INFO mapred.MapTask: Processing split: hdfs://localhost:54310/user/root/wk2/hw25/input/enronemail_1h.txt:0+203954\n",
      "16/01/26 09:43:22 INFO mapred.MapTask: numReduceTasks: 1\n",
      "16/01/26 09:43:22 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "16/01/26 09:43:22 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "16/01/26 09:43:22 INFO mapred.MapTask: soft limit at 83886080\n",
      "16/01/26 09:43:22 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "16/01/26 09:43:22 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "16/01/26 09:43:23 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "16/01/26 09:43:23 INFO streaming.PipeMapRed: PipeMapRed exec [/root/hw2/./mapper2.py]\n",
      "16/01/26 09:43:23 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
      "16/01/26 09:43:23 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
      "16/01/26 09:43:23 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
      "16/01/26 09:43:23 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
      "16/01/26 09:43:23 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
      "16/01/26 09:43:23 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
      "16/01/26 09:43:23 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
      "16/01/26 09:43:23 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
      "16/01/26 09:43:23 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
      "16/01/26 09:43:23 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
      "16/01/26 09:43:23 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
      "16/01/26 09:43:23 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
      "16/01/26 09:43:23 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/26 09:43:23 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/26 09:43:23 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/26 09:43:23 INFO streaming.PipeMapRed: Records R/W=100/1\n",
      "16/01/26 09:43:23 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/01/26 09:43:23 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/01/26 09:43:23 INFO mapred.LocalJobRunner: \n",
      "16/01/26 09:43:23 INFO mapred.MapTask: Starting flush of map output\n",
      "16/01/26 09:43:23 INFO mapred.MapTask: Spilling map output\n",
      "16/01/26 09:43:23 INFO mapred.MapTask: bufstart = 0; bufend = 5166; bufvoid = 104857600\n",
      "16/01/26 09:43:23 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26214000(104856000); length = 397/6553600\n",
      "16/01/26 09:43:23 INFO mapred.MapTask: Finished spill 0\n",
      "16/01/26 09:43:23 INFO mapred.Task: Task:attempt_local1707625718_0001_m_000000_0 is done. And is in the process of committing\n",
      "16/01/26 09:43:23 INFO mapred.LocalJobRunner: Records R/W=100/1\n",
      "16/01/26 09:43:23 INFO mapred.Task: Task 'attempt_local1707625718_0001_m_000000_0' done.\n",
      "16/01/26 09:43:23 INFO mapred.LocalJobRunner: Finishing task: attempt_local1707625718_0001_m_000000_0\n",
      "16/01/26 09:43:23 INFO mapred.LocalJobRunner: map task executor complete.\n",
      "16/01/26 09:43:23 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
      "16/01/26 09:43:23 INFO mapred.LocalJobRunner: Starting task: attempt_local1707625718_0001_r_000000_0\n",
      "16/01/26 09:43:23 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/01/26 09:43:23 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
      "16/01/26 09:43:23 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@564d06b4\n",
      "16/01/26 09:43:23 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=353422528, maxSingleShuffleLimit=88355632, mergeThreshold=233258880, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "16/01/26 09:43:23 INFO reduce.EventFetcher: attempt_local1707625718_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "16/01/26 09:43:23 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1707625718_0001_m_000000_0 decomp: 5368 len: 5372 to MEMORY\n",
      "16/01/26 09:43:23 INFO reduce.InMemoryMapOutput: Read 5368 bytes from map-output for attempt_local1707625718_0001_m_000000_0\n",
      "16/01/26 09:43:23 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 5368, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->5368\n",
      "16/01/26 09:43:23 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "16/01/26 09:43:23 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/01/26 09:43:23 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "16/01/26 09:43:23 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/01/26 09:43:23 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 5343 bytes\n",
      "16/01/26 09:43:23 INFO reduce.MergeManagerImpl: Merged 1 segments, 5368 bytes to disk to satisfy reduce memory limit\n",
      "16/01/26 09:43:23 INFO reduce.MergeManagerImpl: Merging 1 files, 5372 bytes from disk\n",
      "16/01/26 09:43:23 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "16/01/26 09:43:23 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/01/26 09:43:23 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 5343 bytes\n",
      "16/01/26 09:43:23 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/01/26 09:43:23 INFO streaming.PipeMapRed: PipeMapRed exec [/root/hw2/./reducer2.py]\n",
      "16/01/26 09:43:23 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
      "16/01/26 09:43:23 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "16/01/26 09:43:23 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/26 09:43:23 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/26 09:43:23 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/26 09:43:23 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/01/26 09:43:23 INFO streaming.PipeMapRed: Records R/W=100/1\n",
      "16/01/26 09:43:23 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/01/26 09:43:23 INFO mapreduce.Job: Job job_local1707625718_0001 running in uber mode : false\n",
      "16/01/26 09:43:23 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/01/26 09:43:23 INFO mapred.Task: Task:attempt_local1707625718_0001_r_000000_0 is done. And is in the process of committing\n",
      "16/01/26 09:43:23 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/01/26 09:43:23 INFO mapred.Task: Task attempt_local1707625718_0001_r_000000_0 is allowed to commit now\n",
      "16/01/26 09:43:23 INFO output.FileOutputCommitter: Saved output of task 'attempt_local1707625718_0001_r_000000_0' to hdfs://localhost:54310/user/root/wk2/hw25/output_2/_temporary/0/task_local1707625718_0001_r_000000\n",
      "16/01/26 09:43:23 INFO mapred.LocalJobRunner: Records R/W=100/1 > reduce\n",
      "16/01/26 09:43:23 INFO mapred.Task: Task 'attempt_local1707625718_0001_r_000000_0' done.\n",
      "16/01/26 09:43:23 INFO mapred.LocalJobRunner: Finishing task: attempt_local1707625718_0001_r_000000_0\n",
      "16/01/26 09:43:23 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
      "16/01/26 09:43:24 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/01/26 09:43:24 INFO mapreduce.Job: Job job_local1707625718_0001 completed successfully\n",
      "16/01/26 09:43:24 INFO mapreduce.Job: Counters: 35\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=222624\n",
      "\t\tFILE: Number of bytes written=784612\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=407908\n",
      "\t\tHDFS: Number of bytes written=5405\n",
      "\t\tHDFS: Number of read operations=13\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=4\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=100\n",
      "\t\tMap output records=100\n",
      "\t\tMap output bytes=5166\n",
      "\t\tMap output materialized bytes=5372\n",
      "\t\tInput split bytes=117\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=100\n",
      "\t\tReduce shuffle bytes=5372\n",
      "\t\tReduce input records=100\n",
      "\t\tReduce output records=202\n",
      "\t\tSpilled Records=200\n",
      "\t\tShuffled Maps =1\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=1\n",
      "\t\tGC time elapsed (ms)=28\n",
      "\t\tTotal committed heap usage (bytes)=1013448704\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=203954\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=5405\n",
      "16/01/26 09:43:24 INFO streaming.StreamJob: Output directory: /user/root/wk2/hw25/output_2\n"
     ]
    }
   ],
   "source": [
    "# Delete output folder if exists\n",
    "!hdfs dfs -rm -r /user/root/wk2/hw25/output_2\n",
    "\n",
    "# Run Hadoop Streaming job\n",
    "!hadoop jar hadoop-streaming-2.7.1.jar \\\n",
    "-mapper mapper2.py \\\n",
    "-reducer reducer2.py \\\n",
    "-input /user/root/wk2/hw25/input \\\n",
    "-output /user/root/wk2/hw25/output_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/26 09:43:38 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "\t\n",
      "Training error: 1.0%\t\n",
      "Accuracy: 99.0%\t\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat /user/root/wk2/hw25/output_2/part-00000|tail -3"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The training error is 1.0% when we ignored the words with frequency less than 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW 2.6. Benchmark your code with the Python SciKit-Learn implementation of the multinomial Naive Bayes algorithm\n",
    "It always a good idea to benchmark your solutions against publicly available libraries such as SciKit-Learn, The Machine Learning toolkit available in Python. In this exercise, we benchmark ourselves against the SciKit-Learn implementation of multinomial Naive Bayes.  For more information on this implementation see: http://scikit-learn.org/stable/modules/naive_bayes.html more  \n",
    "\n",
    "In this exercise, please complete the following:\n",
    "\n",
    "— Run the Multinomial Naive Bayes algorithm (using default settings) from SciKit-Learn over the same training data used in HW2.5 and report the misclassification error (please note some data preparation might be needed to get the Multinomial Naive Bayes algorithm from SkiKit-Learn to run over this dataset)\n",
    "- Prepare a table to present your results, where rows correspond to approach used (SkiKit-Learn versus your Hadoop implementation) and the column presents the training misclassification error\n",
    "— Explain/justify any differences in terms of training error rates over the dataset in HW2.5 between your Multinomial Naive Bayes implementation (in Map Reduce) versus the Multinomial Naive Bayes implementation in SciKit-Learn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multinomial Bayes Training Error:  0.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
    "import string\n",
    "\n",
    "trainX = []\n",
    "trainY = []\n",
    "\n",
    "# Take only the subject and body here to simulate the mapper code\n",
    "with open('enronemail_1h.txt', 'r') as myfile:\n",
    "    for line in myfile.readlines():\n",
    "        try:\n",
    "            # Tokenize each line. Line format - DOC_ID <tab> SPAM <tab> subject <tab> body\n",
    "            tokens = line.lower().strip().split('\\t')\n",
    "            label = tokens[1]\n",
    "    \n",
    "            # Concatenate subject and body fields and store it in word_string\n",
    "            word_string = tokens[2].strip() + ' ' + tokens[3].strip()\n",
    "    \n",
    "            # Remove punctuation\n",
    "            word_string = word_string.translate(string.maketrans(\"\", \"\"), \n",
    "                                       string.punctuation)\n",
    "    \n",
    "            trainX.append(word_string)\n",
    "        except ValueError:\n",
    "            email_id, label, body = line.split('\\t')\n",
    "            X_train.append(body)\n",
    "        # extract only words from the combined subject and body text\n",
    "        trainY.append(int(label))\n",
    "\n",
    "# Use the TfidVectorizer to create the feature vectors\n",
    "# We should override the tokenizer regular expression to make it the same as what we used\n",
    "# in our poor man's mapper code\n",
    "vectorizer = TfidfVectorizer(token_pattern = \"[\\w']+\")\n",
    "vf = vectorizer.fit(trainX,trainY)\n",
    "\n",
    "clf = MultinomialNB()\n",
    "clf.fit(vf.fit_transform(trainX), trainY)\n",
    "print \"Multinomial Bayes Training Error: \", 1.0 - clf.score(vf.fit_transform(trainX), trainY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### HW 2.6 - Summary of Results\n",
    "\n",
    "| Classification Methodology               | Training error |\n",
    "|------------------------------------------|----------------|\n",
    "| scikit-learn MultinomialNB               |      0%        |\n",
    "| Multinomial NB MapReduce Implementation  |      0%        |\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
